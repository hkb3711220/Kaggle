{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from psutil import cpu_count\n",
    "from torchvision.models import resnet50, vgg19_bn, vgg11_bn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 520\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_argument_clear_large.csv',\n",
       " 'label_argument_clear.csv',\n",
       " 'argument_multi.pickle',\n",
       " 'argument_label_multi.csv',\n",
       " 'argument.pickle',\n",
       " 'argument_large.pickle',\n",
       " 'argument_clear.pickle',\n",
       " 'argument_clear_large.pickle',\n",
       " 'argument_label_large.csv',\n",
       " 'argument_larger.pickle',\n",
       " 'argument_label_larger.csv',\n",
       " 'argument_label.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input/argument-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "dataset_dir = Path('../input/freesound-audio-tagging-2019')\n",
    "preprocessed_dir = Path('../input/fat2019_prep_mels1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = {\n",
    "    'train_curated': dataset_dir / 'train_curated.csv',\n",
    "    'train_noisy': dataset_dir / 'train_noisy.csv',\n",
    "    'sample_submission': dataset_dir / 'sample_submission.csv',\n",
    "    'argument': '../input/argument-data/label_argument_clear_large.csv'\n",
    "}\n",
    "\n",
    "dataset = {\n",
    "    'train_curated': dataset_dir / 'train_curated',\n",
    "    'train_noisy': dataset_dir / 'train_noisy',\n",
    "    'test': dataset_dir / 'test',\n",
    "}\n",
    "\n",
    "mels = {\n",
    "    'train_curated': preprocessed_dir / 'mels_train_curated.pkl',\n",
    "    'train_noisy': preprocessed_dir / 'mels_trn_noisy_best50s.pkl',\n",
    "    'test': preprocessed_dir / 'mels_test.pkl',  # NOTE: this data doesn't work at 2nd stage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006ae4e.wav</td>\n",
       "      <td>Bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0019ef41.wav</td>\n",
       "      <td>Raindrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ec0ad.wav</td>\n",
       "      <td>Finger_snapping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0026c7cb.wav</td>\n",
       "      <td>Run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0026f116.wav</td>\n",
       "      <td>Finger_snapping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname           labels\n",
       "0  0006ae4e.wav             Bark\n",
       "1  0019ef41.wav         Raindrop\n",
       "2  001ec0ad.wav  Finger_snapping\n",
       "3  0026c7cb.wav              Run\n",
       "4  0026f116.wav  Finger_snapping"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_curated = pd.read_csv(csvs['train_curated'])\n",
    "train_noisy   = pd.read_csv(csvs['train_noisy'])\n",
    "train_argument = pd.read_csv(csvs['argument'])\n",
    "train_df      = pd.concat([train_curated, train_noisy], sort=True, ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(csvs['sample_submission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_df.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24785, 80) (5695, 80)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.zeros((len(train_df), num_classes)).astype(int)\n",
    "y_train_argument = np.zeros((len(train_argument), num_classes)).astype(int)\n",
    "\n",
    "for i, row in enumerate(train_df['labels'].str.split(',')):\n",
    "    for label in row:\n",
    "        idx = labels.index(label)\n",
    "        y_train[i, idx] = 1\n",
    "\n",
    "for i, row in enumerate(train_argument['labels'].str.split(',')):\n",
    "    for label in row:\n",
    "        idx = labels.index(label)\n",
    "        y_train_argument[i, idx] = 1\n",
    "        \n",
    "print(y_train.shape, y_train_argument.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24785, 5695)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../input/all-data/train_curated.pickle', 'rb') as curated:\n",
    "    x_train = pickle.load(curated)\n",
    "\n",
    "for i in range(5):\n",
    "    with open('../input/all-data/train_noisy_{}.pickle'.format(i+1), 'rb') as noisy:\n",
    "        x_train.extend(pickle.load(noisy))\n",
    "\n",
    "with open('../input/argument-data/argument_clear_large.pickle', 'rb') as argument:\n",
    "    x_train_argument = pickle.load(argument)\n",
    "\n",
    "len(x_train), len(x_train_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FATTrainDataset(Dataset):\n",
    "    def __init__(self, mels, labels, transforms):\n",
    "        super().__init__()\n",
    "        self.mels = mels\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # crop 1sec\n",
    "        image = Image.fromarray(self.mels[idx], mode='RGB')        \n",
    "        time_dim, base_dim = image.size\n",
    "        crop = random.randint(0, time_dim - base_dim)\n",
    "        image = image.crop([crop, 0, crop + base_dim, base_dim])\n",
    "        image = self.transforms(image).div_(255)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        label = torch.from_numpy(label).float()\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FATTestDataset(Dataset):\n",
    "    def __init__(self, fnames, mels, transforms, tta=5):\n",
    "        super().__init__()\n",
    "        self.fnames = fnames\n",
    "        self.mels = mels\n",
    "        self.transforms = transforms\n",
    "        self.tta = tta\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames) * self.tta\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        new_idx = idx % len(self.fnames)\n",
    "        \n",
    "        image = Image.fromarray(self.mels[new_idx], mode='RGB')\n",
    "        time_dim, base_dim = image.size\n",
    "        crop = random.randint(0, time_dim - base_dim)\n",
    "        image = image.crop([crop, 0, crop + base_dim, base_dim])\n",
    "        image = self.transforms(image).div_(255)\n",
    "\n",
    "        fname = self.fnames[new_idx]\n",
    "        \n",
    "        return image, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_dict = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate_ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, drop_rate=0.0):\n",
    "        super(Gate_ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool  = nn.MaxPool2d(2, stride=2)\n",
    "        self.drop_rate = drop_rate\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        layer1_L      = self.conv1(x)\n",
    "        layer1_L      = self.bn1(layer1_L)\n",
    "        layer1_S      = torch.sigmoid(layer1_L)\n",
    "        layer1_output = layer1_L * layer1_S\n",
    "        layer2_L      = self.conv2(layer1_output)\n",
    "        layer2_L      = self.bn2(layer2_L)\n",
    "        layer2_S      = torch.sigmoid(layer2_L)\n",
    "        layer2_output = layer2_L * layer2_S\n",
    "        output        = self.pool(layer2_output)\n",
    "        \n",
    "        if self.drop_rate > 0:\n",
    "            return F.dropout(output, p=self.drop_rate, training=self.training)\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "class CLDNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CLDNN, self).__init__()\n",
    "        self.prepreocess = nn.Sequential(nn.BatchNorm2d(3), nn.ReLU())\n",
    "        self.cnn = nn.Sequential(Gate_ConvBlock(3, 64),   #(N, 64, 64, 64) #Because overfit trainset in early stage, i add dropout.\n",
    "                                 Gate_ConvBlock(64, 128), #(N, 128, 32, 32)\n",
    "                                 Gate_ConvBlock(128, 256),#(N, 256, 16, 16)\n",
    "                                 Gate_ConvBlock(256, 512),#(N, 512, 8, 8)\n",
    "                                 Gate_ConvBlock(512, 512))#(N, 512, 4, 4)\n",
    "        self.gru = nn.GRU(512*4, 512, batch_first=True, bidirectional=True) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(1024, 80))\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.prepreocess(x)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), x.size(1)*x.size(2), x.size(-1)) #(N, 2048, 4)  \n",
    "        x = x.permute((0, 2, 1)) #(N, 4, 2048)\n",
    "        x, _ = self.gru(x) #(N, 4, 1024)\n",
    "        x = self.relu(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = x.view(x.size(0), -1) #(N, 1024)\n",
    "        x = self.fc(x) #(N, 80)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftBootstrappingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/1901.01189.pdf\n",
    "    Loss(t, p) = - (beta * t + (1 - beta) * p) * log(p)\n",
    "    The idea is to payless attention to the noisy labels, \n",
    "    in favour of the model predictions,which are more reliable as the learning progresses\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=0.95, reduce=True):\n",
    "        super(SoftBootstrappingLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # cross_entropy = - t * log(p)\n",
    "        beta_xentropy = self.beta * torch.sum(F.binary_cross_entropy_with_logits(input, target, reduction='none'), dim=1)\n",
    "        \n",
    "        # second term = - (1 - beta) * p * log(p)\n",
    "        bootstrap = - (1.0 - self.beta) * torch.sum(torch.sigmoid(input) * F.logsigmoid(input), dim=1)\n",
    "        \n",
    "        if self.reduce:\n",
    "            return torch.mean(beta_xentropy + bootstrap)\n",
    "        \n",
    "        return beta_xentropy + bootstrap\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, train_index, val_index, n_fold, train_transforms, enable_checkpoint_ensemble=False):\n",
    "    \n",
    "    num_epochs = 120\n",
    "    batch_size = 128\n",
    "    test_batch_size = 128\n",
    "    lr = 1e-3\n",
    "    eta_min = 1e-5\n",
    "    t_max = 5\n",
    "    \n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    x_trn= [x_train[idx] for idx in train_index] + x_train_argument\n",
    "    x_val= [x_train[idx] for idx in val_index]\n",
    "    y_trn= np.asarray([y_train[idx] for idx in train_index])\n",
    "    y_trn= np.vstack((y_trn, y_train_argument))\n",
    "    y_val= [y_train[idx] for idx in val_index]\n",
    "    \n",
    "    train_dataset = FATTrainDataset(x_trn, y_trn, train_transforms)\n",
    "    valid_dataset = FATTrainDataset(x_val, y_val, train_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "    \n",
    "    model = CLDNN().cuda()    \n",
    "    criterion = SoftBootstrappingLoss(beta=0.3).cuda()\n",
    "    \n",
    "    \"\"\"\n",
    "    An Effective Label Noise Model for DNN Text Classiï¬cation\n",
    "    https://arxiv.org/abs/1903.07507\n",
    "    \n",
    "    find the l2 regularization with a small penaltyworks better than a large penalty since, \n",
    "    for low label noise, learning a less diffuse noise is beneficial.\n",
    "    without regulariza-tion, the noise model has less ability to diffuse the diagonal elements which leads to poor classifica-tion performance.\"\"\"\n",
    "    optimizer = Adam(params=model.parameters(), lr=lr, amsgrad=False, weight_decay=1e-6)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_lwlrap = 0.\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        checkpoint_weights = [2 ** epoch for epoch in range(epoch+1)]\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            preds = model(x_batch.cuda())\n",
    "            loss = criterion(preds, y_batch.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(x_val), num_classes))\n",
    "        avg_val_loss = 0.\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            preds = model(x_batch.cuda()).detach()\n",
    "            loss = criterion(preds, y_batch.cuda())\n",
    "            preds = torch.sigmoid(preds)\n",
    "            valid_preds[i * test_batch_size: (i+1) * test_batch_size] = preds.cpu().numpy()\n",
    "            \n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "        score, weight = calculate_per_class_lwlrap(np.asarray(y_val), valid_preds)\n",
    "        lwlrap = (score * weight).sum()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  val_lwlrap: {lwlrap:.6f}  time: {elapsed:.0f}s')\n",
    "    \n",
    "        if lwlrap > best_lwlrap:\n",
    "            best_epoch = epoch + 1\n",
    "            best_lwlrap = lwlrap\n",
    "            torch.save(model.state_dict(), 'weight_best_{}.pt'.format(n_fold))\n",
    "            \n",
    "    return {\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_lwlrap': best_lwlrap,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = pd.read_csv('../input/train-stratified/train_stratified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 3.8355  avg_val_loss: 2.9226  val_lwlrap: 0.076550  time: 142s\n",
      "Epoch 2 - avg_train_loss: 3.2111  avg_val_loss: 2.7788  val_lwlrap: 0.150974  time: 141s\n",
      "Epoch 3 - avg_train_loss: 2.8954  avg_val_loss: 2.6036  val_lwlrap: 0.248914  time: 141s\n",
      "Epoch 4 - avg_train_loss: 2.6832  avg_val_loss: 2.5086  val_lwlrap: 0.297426  time: 141s\n",
      "Epoch 5 - avg_train_loss: 2.5343  avg_val_loss: 2.4348  val_lwlrap: 0.338809  time: 141s\n",
      "Epoch 6 - avg_train_loss: 2.4300  avg_val_loss: 2.3568  val_lwlrap: 0.370708  time: 141s\n",
      "Epoch 7 - avg_train_loss: 2.3912  avg_val_loss: 2.3462  val_lwlrap: 0.380122  time: 141s\n",
      "Epoch 8 - avg_train_loss: 2.3816  avg_val_loss: 2.3415  val_lwlrap: 0.382427  time: 141s\n",
      "Epoch 9 - avg_train_loss: 2.3714  avg_val_loss: 2.3510  val_lwlrap: 0.377465  time: 141s\n",
      "Epoch 10 - avg_train_loss: 2.3428  avg_val_loss: 2.3604  val_lwlrap: 0.371075  time: 141s\n",
      "Epoch 11 - avg_train_loss: 2.2768  avg_val_loss: 2.4301  val_lwlrap: 0.347978  time: 141s\n",
      "Epoch 12 - avg_train_loss: 2.1925  avg_val_loss: 2.2578  val_lwlrap: 0.424435  time: 141s\n",
      "Epoch 13 - avg_train_loss: 2.1045  avg_val_loss: 2.3284  val_lwlrap: 0.399443  time: 141s\n",
      "Epoch 14 - avg_train_loss: 1.9993  avg_val_loss: 2.2237  val_lwlrap: 0.442361  time: 141s\n",
      "Epoch 15 - avg_train_loss: 1.9000  avg_val_loss: 2.0404  val_lwlrap: 0.516283  time: 141s\n",
      "Epoch 16 - avg_train_loss: 1.8221  avg_val_loss: 2.0092  val_lwlrap: 0.522764  time: 140s\n",
      "Epoch 17 - avg_train_loss: 1.7870  avg_val_loss: 1.9827  val_lwlrap: 0.533288  time: 140s\n",
      "Epoch 18 - avg_train_loss: 1.7860  avg_val_loss: 1.9889  val_lwlrap: 0.524430  time: 140s\n",
      "Epoch 19 - avg_train_loss: 1.8136  avg_val_loss: 2.0379  val_lwlrap: 0.514111  time: 140s\n",
      "Epoch 20 - avg_train_loss: 1.8507  avg_val_loss: 2.0824  val_lwlrap: 0.489330  time: 140s\n",
      "Epoch 21 - avg_train_loss: 1.8483  avg_val_loss: 2.1502  val_lwlrap: 0.472722  time: 140s\n",
      "Epoch 22 - avg_train_loss: 1.8207  avg_val_loss: 2.1354  val_lwlrap: 0.474305  time: 140s\n",
      "Epoch 23 - avg_train_loss: 1.7553  avg_val_loss: 1.9850  val_lwlrap: 0.529739  time: 140s\n",
      "Epoch 24 - avg_train_loss: 1.6590  avg_val_loss: 1.9694  val_lwlrap: 0.535067  time: 140s\n",
      "Epoch 25 - avg_train_loss: 1.5634  avg_val_loss: 1.8723  val_lwlrap: 0.566857  time: 140s\n",
      "Epoch 26 - avg_train_loss: 1.4981  avg_val_loss: 1.8177  val_lwlrap: 0.582196  time: 140s\n",
      "Epoch 27 - avg_train_loss: 1.4667  avg_val_loss: 1.7962  val_lwlrap: 0.590867  time: 140s\n",
      "Epoch 28 - avg_train_loss: 1.4652  avg_val_loss: 1.8171  val_lwlrap: 0.581228  time: 140s\n",
      "Epoch 29 - avg_train_loss: 1.4928  avg_val_loss: 1.8566  val_lwlrap: 0.571844  time: 140s\n",
      "Epoch 30 - avg_train_loss: 1.5472  avg_val_loss: 1.9434  val_lwlrap: 0.540635  time: 140s\n",
      "Epoch 31 - avg_train_loss: 1.5790  avg_val_loss: 2.0166  val_lwlrap: 0.524898  time: 140s\n",
      "Epoch 32 - avg_train_loss: 1.5626  avg_val_loss: 1.9946  val_lwlrap: 0.535580  time: 140s\n",
      "Epoch 33 - avg_train_loss: 1.5110  avg_val_loss: 1.9795  val_lwlrap: 0.536680  time: 140s\n",
      "Epoch 34 - avg_train_loss: 1.4269  avg_val_loss: 1.8359  val_lwlrap: 0.578481  time: 140s\n",
      "Epoch 35 - avg_train_loss: 1.3353  avg_val_loss: 1.8270  val_lwlrap: 0.581367  time: 140s\n",
      "Epoch 36 - avg_train_loss: 1.2605  avg_val_loss: 1.7523  val_lwlrap: 0.601429  time: 140s\n",
      "Epoch 37 - avg_train_loss: 1.2329  avg_val_loss: 1.7372  val_lwlrap: 0.610603  time: 140s\n",
      "Epoch 38 - avg_train_loss: 1.2340  avg_val_loss: 1.7535  val_lwlrap: 0.605187  time: 140s\n",
      "Epoch 39 - avg_train_loss: 1.2671  avg_val_loss: 1.7974  val_lwlrap: 0.590426  time: 140s\n",
      "Epoch 40 - avg_train_loss: 1.3226  avg_val_loss: 1.8627  val_lwlrap: 0.569845  time: 140s\n",
      "Epoch 41 - avg_train_loss: 1.3627  avg_val_loss: 1.8518  val_lwlrap: 0.584646  time: 140s\n",
      "Epoch 42 - avg_train_loss: 1.3633  avg_val_loss: 1.9184  val_lwlrap: 0.555168  time: 140s\n",
      "Epoch 43 - avg_train_loss: 1.3133  avg_val_loss: 1.8876  val_lwlrap: 0.572512  time: 141s\n",
      "Epoch 44 - avg_train_loss: 1.2327  avg_val_loss: 1.8417  val_lwlrap: 0.579329  time: 140s\n",
      "Epoch 45 - avg_train_loss: 1.1404  avg_val_loss: 1.8054  val_lwlrap: 0.597071  time: 140s\n",
      "Epoch 46 - avg_train_loss: 1.0691  avg_val_loss: 1.7426  val_lwlrap: 0.616544  time: 140s\n",
      "Epoch 47 - avg_train_loss: 1.0392  avg_val_loss: 1.7319  val_lwlrap: 0.613753  time: 140s\n",
      "Epoch 48 - avg_train_loss: 1.0414  avg_val_loss: 1.7281  val_lwlrap: 0.621744  time: 141s\n",
      "Epoch 49 - avg_train_loss: 1.0768  avg_val_loss: 1.7917  val_lwlrap: 0.601800  time: 141s\n",
      "Epoch 50 - avg_train_loss: 1.1398  avg_val_loss: 1.8435  val_lwlrap: 0.593307  time: 141s\n",
      "Epoch 51 - avg_train_loss: 1.1859  avg_val_loss: 1.8793  val_lwlrap: 0.579345  time: 140s\n",
      "Epoch 52 - avg_train_loss: 1.1849  avg_val_loss: 1.9919  val_lwlrap: 0.556050  time: 140s\n",
      "Epoch 53 - avg_train_loss: 1.1344  avg_val_loss: 1.9248  val_lwlrap: 0.572763  time: 140s\n",
      "Epoch 54 - avg_train_loss: 1.0562  avg_val_loss: 1.8408  val_lwlrap: 0.590275  time: 140s\n",
      "Epoch 55 - avg_train_loss: 0.9665  avg_val_loss: 1.7873  val_lwlrap: 0.606602  time: 141s\n",
      "Epoch 56 - avg_train_loss: 0.8831  avg_val_loss: 1.8067  val_lwlrap: 0.607001  time: 141s\n",
      "Epoch 57 - avg_train_loss: 0.8532  avg_val_loss: 1.7875  val_lwlrap: 0.615801  time: 141s\n",
      "Epoch 58 - avg_train_loss: 0.8593  avg_val_loss: 1.7844  val_lwlrap: 0.615454  time: 141s\n",
      "Epoch 59 - avg_train_loss: 0.8987  avg_val_loss: 1.8202  val_lwlrap: 0.610989  time: 141s\n",
      "Epoch 60 - avg_train_loss: 0.9512  avg_val_loss: 1.9017  val_lwlrap: 0.594885  time: 141s\n",
      "Epoch 61 - avg_train_loss: 1.0121  avg_val_loss: 1.9006  val_lwlrap: 0.596843  time: 141s\n",
      "Epoch 62 - avg_train_loss: 1.0169  avg_val_loss: 1.9909  val_lwlrap: 0.575852  time: 141s\n",
      "Epoch 63 - avg_train_loss: 0.9795  avg_val_loss: 1.9641  val_lwlrap: 0.568710  time: 141s\n",
      "Epoch 64 - avg_train_loss: 0.8868  avg_val_loss: 2.0013  val_lwlrap: 0.577773  time: 141s\n",
      "Epoch 65 - avg_train_loss: 0.7973  avg_val_loss: 1.9024  val_lwlrap: 0.601353  time: 141s\n",
      "Epoch 66 - avg_train_loss: 0.7291  avg_val_loss: 1.8309  val_lwlrap: 0.622598  time: 141s\n",
      "Epoch 67 - avg_train_loss: 0.7001  avg_val_loss: 1.8640  val_lwlrap: 0.617124  time: 141s\n",
      "Epoch 68 - avg_train_loss: 0.6956  avg_val_loss: 1.8683  val_lwlrap: 0.614283  time: 141s\n",
      "Epoch 69 - avg_train_loss: 0.7349  avg_val_loss: 1.9005  val_lwlrap: 0.605856  time: 141s\n",
      "Epoch 70 - avg_train_loss: 0.7928  avg_val_loss: 1.9763  val_lwlrap: 0.586310  time: 141s\n",
      "Epoch 71 - avg_train_loss: 0.8521  avg_val_loss: 1.9697  val_lwlrap: 0.587920  time: 141s\n",
      "Epoch 72 - avg_train_loss: 0.8724  avg_val_loss: 2.0140  val_lwlrap: 0.574988  time: 141s\n",
      "Epoch 73 - avg_train_loss: 0.8226  avg_val_loss: 2.1033  val_lwlrap: 0.573936  time: 141s\n",
      "Epoch 74 - avg_train_loss: 0.7429  avg_val_loss: 2.0290  val_lwlrap: 0.587675  time: 141s\n",
      "Epoch 75 - avg_train_loss: 0.6523  avg_val_loss: 1.9836  val_lwlrap: 0.600219  time: 141s\n",
      "Epoch 76 - avg_train_loss: 0.5889  avg_val_loss: 1.9741  val_lwlrap: 0.612770  time: 141s\n",
      "Epoch 77 - avg_train_loss: 0.5587  avg_val_loss: 1.9572  val_lwlrap: 0.607819  time: 141s\n",
      "Epoch 78 - avg_train_loss: 0.5622  avg_val_loss: 1.9773  val_lwlrap: 0.613483  time: 141s\n",
      "Epoch 79 - avg_train_loss: 0.5934  avg_val_loss: 1.9870  val_lwlrap: 0.608720  time: 141s\n",
      "Epoch 80 - avg_train_loss: 0.6477  avg_val_loss: 2.0452  val_lwlrap: 0.598865  time: 141s\n",
      "Epoch 81 - avg_train_loss: 0.7212  avg_val_loss: 2.1087  val_lwlrap: 0.572086  time: 141s\n",
      "Epoch 82 - avg_train_loss: 0.7425  avg_val_loss: 2.1303  val_lwlrap: 0.575274  time: 141s\n",
      "Epoch 83 - avg_train_loss: 0.6945  avg_val_loss: 2.1520  val_lwlrap: 0.589128  time: 141s\n",
      "Epoch 84 - avg_train_loss: 0.6225  avg_val_loss: 2.1659  val_lwlrap: 0.584683  time: 142s\n",
      "Epoch 85 - avg_train_loss: 0.5384  avg_val_loss: 2.1686  val_lwlrap: 0.589592  time: 142s\n",
      "Epoch 86 - avg_train_loss: 0.4706  avg_val_loss: 2.1130  val_lwlrap: 0.597057  time: 141s\n",
      "Epoch 87 - avg_train_loss: 0.4489  avg_val_loss: 2.1113  val_lwlrap: 0.603012  time: 142s\n",
      "Epoch 88 - avg_train_loss: 0.4523  avg_val_loss: 2.1320  val_lwlrap: 0.598932  time: 142s\n",
      "Epoch 89 - avg_train_loss: 0.4745  avg_val_loss: 2.1736  val_lwlrap: 0.589627  time: 142s\n",
      "Epoch 90 - avg_train_loss: 0.5287  avg_val_loss: 2.2132  val_lwlrap: 0.572805  time: 142s\n",
      "Epoch 91 - avg_train_loss: 0.5966  avg_val_loss: 2.2588  val_lwlrap: 0.572273  time: 142s\n",
      "Epoch 92 - avg_train_loss: 0.6301  avg_val_loss: 2.2594  val_lwlrap: 0.578997  time: 142s\n",
      "Epoch 93 - avg_train_loss: 0.5910  avg_val_loss: 2.3731  val_lwlrap: 0.544858  time: 142s\n",
      "Epoch 94 - avg_train_loss: 0.5098  avg_val_loss: 2.2293  val_lwlrap: 0.590696  time: 142s\n",
      "Epoch 95 - avg_train_loss: 0.4301  avg_val_loss: 2.2401  val_lwlrap: 0.594373  time: 142s\n",
      "Epoch 96 - avg_train_loss: 0.3801  avg_val_loss: 2.2397  val_lwlrap: 0.596640  time: 142s\n",
      "Epoch 97 - avg_train_loss: 0.3619  avg_val_loss: 2.2893  val_lwlrap: 0.594317  time: 142s\n",
      "Epoch 98 - avg_train_loss: 0.3545  avg_val_loss: 2.2891  val_lwlrap: 0.595332  time: 142s\n",
      "Epoch 99 - avg_train_loss: 0.3837  avg_val_loss: 2.2881  val_lwlrap: 0.586543  time: 142s\n",
      "Epoch 100 - avg_train_loss: 0.4354  avg_val_loss: 2.3981  val_lwlrap: 0.576083  time: 142s\n",
      "Epoch 101 - avg_train_loss: 0.5065  avg_val_loss: 2.3665  val_lwlrap: 0.571268  time: 142s\n",
      "Epoch 102 - avg_train_loss: 0.5370  avg_val_loss: 2.3811  val_lwlrap: 0.556972  time: 142s\n",
      "Epoch 103 - avg_train_loss: 0.5100  avg_val_loss: 2.3257  val_lwlrap: 0.565709  time: 142s\n",
      "Epoch 104 - avg_train_loss: 0.4251  avg_val_loss: 2.3743  val_lwlrap: 0.576388  time: 142s\n",
      "Epoch 105 - avg_train_loss: 0.3524  avg_val_loss: 2.4141  val_lwlrap: 0.588337  time: 142s\n",
      "Epoch 106 - avg_train_loss: 0.3129  avg_val_loss: 2.4312  val_lwlrap: 0.587129  time: 141s\n",
      "Epoch 107 - avg_train_loss: 0.2865  avg_val_loss: 2.3529  val_lwlrap: 0.599447  time: 142s\n",
      "Epoch 108 - avg_train_loss: 0.2847  avg_val_loss: 2.3957  val_lwlrap: 0.595938  time: 142s\n",
      "Epoch 109 - avg_train_loss: 0.3091  avg_val_loss: 2.3957  val_lwlrap: 0.590924  time: 142s\n",
      "Epoch 110 - avg_train_loss: 0.3614  avg_val_loss: 2.4813  val_lwlrap: 0.577469  time: 142s\n",
      "Epoch 111 - avg_train_loss: 0.4359  avg_val_loss: 2.4913  val_lwlrap: 0.570733  time: 142s\n",
      "Epoch 112 - avg_train_loss: 0.4632  avg_val_loss: 2.4927  val_lwlrap: 0.558302  time: 142s\n",
      "Epoch 113 - avg_train_loss: 0.4406  avg_val_loss: 2.4247  val_lwlrap: 0.569781  time: 142s\n",
      "Epoch 114 - avg_train_loss: 0.3686  avg_val_loss: 2.5219  val_lwlrap: 0.569467  time: 142s\n",
      "Epoch 115 - avg_train_loss: 0.2938  avg_val_loss: 2.5364  val_lwlrap: 0.582611  time: 142s\n",
      "Epoch 116 - avg_train_loss: 0.2529  avg_val_loss: 2.4772  val_lwlrap: 0.589852  time: 142s\n",
      "Epoch 117 - avg_train_loss: 0.2362  avg_val_loss: 2.4873  val_lwlrap: 0.591613  time: 142s\n",
      "Epoch 118 - avg_train_loss: 0.2330  avg_val_loss: 2.5156  val_lwlrap: 0.587937  time: 142s\n",
      "Epoch 119 - avg_train_loss: 0.2531  avg_val_loss: 2.5402  val_lwlrap: 0.581617  time: 142s\n",
      "Epoch 120 - avg_train_loss: 0.2919  avg_val_loss: 2.6244  val_lwlrap: 0.575622  time: 142s\n",
      "{'best_epoch': 66, 'best_lwlrap': 0.6225978838690955}\n"
     ]
    }
   ],
   "source": [
    "FOLD_NO = [0]\n",
    "for FOLD in FOLD_NO:\n",
    "    val_index = train_s[train_s.fold == FOLD].index\n",
    "    tra_index = train_s[train_s.fold != FOLD].index\n",
    "    result = train_model(x_train, y_train, tra_index, val_index, FOLD, transforms_dict['train'])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predict_model(test_fnames, x_test, test_transforms, num_classes, n_fold, tta=35):\n",
    "    batch_size = 256\n",
    "\n",
    "    test_dataset = FATTestDataset(test_fnames, x_test, test_transforms, tta=tta)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = CLDNN().cuda()\n",
    "    model.load_state_dict(torch.load('../input/feature-level-attention/weight_best_{}.pt'.format(n_fold)))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs, all_fnames = [], []\n",
    "\n",
    "    pb = progress_bar(test_loader)\n",
    "    for images, fnames in pb:\n",
    "        preds = torch.sigmoid(model(images.cuda()).detach())\n",
    "        all_outputs.append(preds.cpu().numpy())\n",
    "        all_fnames.extend(fnames)\n",
    "\n",
    "    test_preds = pd.DataFrame(data=np.concatenate(all_outputs),\n",
    "                              index=all_fnames,\n",
    "                              columns=map(str, range(num_classes)))\n",
    "    test_preds = test_preds.groupby(level=0).mean()\n",
    "\n",
    "    return test_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
