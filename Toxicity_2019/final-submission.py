from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import sys
package_dir = "../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT/"
sys.path.append(package_dir)

import torch.utils.data
import numpy as np
import pandas as pd
import os
import re
import warnings
import time
import random
from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam
from pytorch_pretrained_bert import BertConfig
import torch.nn as nn
from nltk.tokenize.treebank import TreebankWordTokenizer
from tqdm import tqdm
warnings.filterwarnings(action='ignore')
device = torch.device('cuda')

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

class config:
    ##COMMON CONFIG
    PATH = '../input'
    CSV  = {'test': os.path.join(PATH, 'jigsaw-unintended-bias-in-toxicity-classification/test.csv'),
            'sample_submission':os.path.join(PATH, 'jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')}
    SEED = 1234
    BATCH_SIZE = 32
    MAX_SEQUENCE_LENGTH = 220
    ##BERT
    BERT_MODEL_PATH = {'base': '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/',
                       'large': '../input/bert-pretrained-models/uncased_l-24_1024_a-16/uncased_L-24_H-1024_A-16/'}
    BERT_VOCAB = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/vocab.txt'
    BERT_MODEL_WEIGHT    = {'bert_1': os.path.join(PATH, 'bert-test-1'),
                            'bert_2': os.path.join(PATH, 'bert-test-8')}
    
    ##GPT2
    GPT_MODEL_PATH = '../input/gpt2-models/'
    GPT_MODEL_WEIGHT = '../input/gpt2-p1e2-lr10/gpt2_model_lr1.0.bin'
    
    
    ##LSTM
    WORD_INDEX_PATH = '../input/best-rnn/word_index.pk'
    RNN_PATH = '../input/best-rnn/'
    CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'
    GLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'
    NUM_MODELS = 2
    LSTM_UNITS = 128
    DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS
    MAX_LSTM_LENGTH = 300
    MAX_FEATURES = 400000

contraction_mapping = {
    "Trump's" : 'trump is', "trump's" : 'trump is', "'cause": 'because',',cause': 'because',';cause': 'because',"ain't": 'am not','ain,t': 'am not',
    'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',"aren't": 'are not',
    'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',"can't": 'cannot',"can't've": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',
    'can;t': 'cannot','can;t;ve': 'cannot have',
    'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',
    "could've": 'could have','could,ve': 'could have','could;ve': 'could have',"couldn't": 'could not',"couldn't've": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',
    'couldn;t;ve': 'could not have','couldnÂ´t': 'could not',
    'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not','couldnâ€™tâ€™ve': 'could not have','couldÂ´ve': 'could have',
    'couldâ€™ve': 'could have',"didn't": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',
    'didnâ€™t': 'did not',"doesn't": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',
    'doesnâ€™t': 'does not',"don't": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',
    "hadn't": 'had not',"hadn't've": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',
    'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not','hadnâ€™tâ€™ve': 'had not have',"hasn't": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not','hasnâ€™t': 'has not',
    "haven't": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not','havenâ€™t': 'have not',"he'd": 'he would',
    "he'd've": 'he would have',"he'll": 'he will',
    "he's": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',
    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have','heÂ´ll': 'he will',
    'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',"how'd": 'how did',"how'll": 'how will',
    "how's": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',
    'how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will','howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will',
    'howâ€™s': 'how is',"i'd": 'i would',"i'll": 'i will',"i'm": 'i am',"i've": 'i have','i,d': 'i would','i,ll': 'i will',
    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',"isn't": 'is not',
    'isn,t': 'is not','isn;t': 'is not','isnÂ´t': 'is not','isnâ€™t': 'is not',"it'd": 'it would',"it'll": 'it will',"It's":'it is',
    "it's": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is',
    'itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',
    'iÂ´d': 'i would','IÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',
    'iâ€™ve': 'i have',"let's": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us',
    'letâ€™s': 'let us',"ma'am": 'madam','ma,am': 'madam','ma;am': 'madam',"mayn't": 'may not','mayn,t': 'may not','mayn;t': 'may not',
    'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',"might've": 'might have','might,ve': 'might have','might;ve': 'might have',"mightn't": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightnÂ´t': 'might not',
    'mightnâ€™t': 'might not','mightÂ´ve': 'might have','mightâ€™ve': 'might have',"must've": 'must have','must,ve': 'must have','must;ve': 'must have',
    "mustn't": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not','mustÂ´ve': 'must have',
    'mustâ€™ve': 'must have',"needn't": 'need not','needn,t': 'need not','needn;t': 'need not','neednÂ´t': 'need not','neednâ€™t': 'need not',"oughtn't": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',
    'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',"sha'n't": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',"shan't": 'shall not',
    'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not','shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',
    "she'd": 'she would',"she'll": 'she will',"she's": 'she is','she,d': 'she would','she,ll': 'she will',
    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will',
    'sheÂ´s': 'she is','sheâ€™d': 'she would','sheâ€™ll': 'she will','sheâ€™s': 'she is',"should've": 'should have','should,ve': 'should have','should;ve': 'should have',
    "shouldn't": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have',
    'shouldâ€™ve': 'should have',"that'd": 'that would',"that's": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',
    'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',"there'd": 'there had',
    "there's": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',
    'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',
    "they'd": 'they would',"they'll": 'they will',"they're": 'they are',"they've": 'they have',
    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',
    'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are','theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will',
    'theyâ€™re': 'they are','theyâ€™ve': 'they have',"wasn't": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not',
    'wasnâ€™t': 'was not',"we'd": 'we would',"we'll": 'we will',"we're": 'we are',"we've": 'we have','we,d': 'we would','we,ll': 'we will',
    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',
    "weren't": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not','weÂ´d': 'we would','weÂ´ll': 'we will',
    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would','weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',"what'll": 'what will',"what're": 'what are',"what's": 'what is',
    "what've": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',
    'what;s': 'what is','what;ve': 'what have','whatÂ´ll': 'what will',
    'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will','whatâ€™re': 'what are','whatâ€™s': 'what is',
    'whatâ€™ve': 'what have',"where'd": 'where did',"where's": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',
    'where;s': 'where is','whereÂ´d': 'where did','whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is',
    "who'll": 'who will',"who's": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',
    'whoÂ´ll': 'who will','whoÂ´s': 'who is','whoâ€™ll': 'who will','whoâ€™s': 'who is',"won't": 'will not','won,t': 'will not','won;t': 'will not',
    'wonÂ´t': 'will not','wonâ€™t': 'will not',"wouldn't": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldnÂ´t': 'would not',
    'wouldnâ€™t': 'would not',"you'd": 'you would',"you'll": 'you will',"you're": 'you are','you,d': 'you would','you,ll': 'you will',
    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',
    'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would','youâ€™ll': 'you will','youâ€™re': 'you are',
    'Â´cause': 'because','â€™cause': 'because',"you've": "you have","could'nt": 'could not',
    "havn't": 'have not',"hereâ€™s": "here is",'i""m': 'i am',"i'am": 'i am',"i'l": "i will","i'v": 'i have',"wan't": 'want',"was'nt": "was not","who'd": "who would",
    "who're": "who are","who've": "who have","why'd": "why would","would've": "would have","y'all": "you all","y'know": "you know","you.i": "you i",
    "your'e": "you are","arn't": "are not","agains't": "against","c'mon": "common","doens't": "does not",'don""t': "do not","dosen't": "does not",
    "dosn't": "does not","shoudn't": "should not","that'll": "that will","there'll": "there will","there're": "there are",
    "this'll": "this all","u're": "you are", "ya'll": "you all","you'r": "you are","youâ€™ve": "you have","d'int": "did not","did'nt": "did not","din't": "did not","dont't": "do not","gov't": "government",
    "i'ma": "i am","is'nt": "is not","â€˜I":'I',
    'á´€É´á´…':'and','á´›Êœá´‡':'the','Êœá´á´á´‡':'home','á´œá´˜':'up','Ê™Ê':'by','á´€á´›':'at','â€¦and':'and','civilbeat':'civil beat',\
    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','á´„Êœá´‡á´„á´‹':'check','Ò“á´Ê€':'for','á´›ÊœÉªs':'this','á´„á´á´á´˜á´œá´›á´‡Ê€':'computer',\
    'á´á´É´á´›Êœ':'month','á´¡á´Ê€á´‹ÉªÉ´É¢':'working','á´Šá´Ê™':'job','Ò“Ê€á´á´':'from','Sá´›á´€Ê€á´›':'start','gubmit':'submit','COâ‚‚':'carbon dioxide','Ò“ÉªÊ€sá´›':'first',\
    'á´‡É´á´…':'end','á´„á´€É´':'can','Êœá´€á´ á´‡':'have','á´›á´':'to','ÊŸÉªÉ´á´‹':'link','á´Ò“':'of','Êœá´á´œÊ€ÊŸÊ':'hourly','á´¡á´‡á´‡á´‹':'week','á´‡É´á´…':'end','á´‡xá´›Ê€á´€':'extra',\
    'GÊ€á´‡á´€á´›':'great','sá´›á´œá´…á´‡É´á´›s':'student','sá´›á´€Ê':'stay','á´á´á´s':'mother','á´Ê€':'or','á´€É´Êá´É´á´‡':'anyone','É´á´‡á´‡á´…ÉªÉ´É¢':'needing','á´€É´':'an','ÉªÉ´á´„á´á´á´‡':'income',\
    'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡':'reliable','Ò“ÉªÊ€sá´›':'first','Êá´á´œÊ€':'your','sÉªÉ¢É´ÉªÉ´É¢':'signing','Ê™á´á´›á´›á´á´':'bottom','Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢':'following','Má´€á´‹á´‡':'make',\
    'á´„á´É´É´á´‡á´„á´›Éªá´É´':'connection','ÉªÉ´á´›á´‡Ê€É´á´‡á´›':'internet','financialpost':'financial post', 'Êœaá´ á´‡':' have ', 'á´„aÉ´':' can ', 'Maá´‹á´‡':' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡':' reliable ', 'É´á´‡á´‡á´…':' need ',
    'á´É´ÊŸÊ':' only ', 'á´‡xá´›Ê€a':' extra ', 'aÉ´':' an ', 'aÉ´Êá´É´á´‡':' anyone ', 'sá´›aÊ':' stay ', 'Sá´›aÊ€á´›':' start', 'SHOPO':'shop',
    }

punct = "/-'?!.,#$%\'()*+-/:;<=>@[\\]^_`{|}~" + '""â€œâ€â€™' + 'âˆÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\Ã—â„¢âˆšÂ²â€”â€“&'
punct_mapping = {"â€˜": "'", "â‚¹": "e", "Â´": "'", "Â°": "", "â‚¬": "e", "â„¢": "tm", "âˆš": " sqrt ", "Ã—": "x", "Â²": "2", "â€”": "-", "â€“": "-", "â€™": "'", "_": "-", "`": "'", 'â€œ': '"', 'â€': '"', 'â€œ': '"', "Â£": "e", 'âˆ': 'infinity', 'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'Î²': 'beta', 'âˆ…': '', 'Â³': '3', 'Ï€': 'pi', }
specials = {'\u200b': ' ', 'â€¦': ' ... ', '\ufeff': '', 'à¤•à¤°à¤¨à¤¾': '', 'à¤¹à¥ˆ': ''}  # Other special characters that I have to deal with in last
small_caps_mapping = {
    "á´€": "a", "Ê™": "b", "á´„": "c", "á´…": "d", "á´‡": "e", "Ò“": "f", "É¢": "g", "Êœ": "h", "Éª": "i", 
    "á´Š": "j", "á´‹": "k", "ÊŸ": "l", "á´": "m", "É´": "n", "á´": "o", "á´˜": "p", "Ç«": "q", "Ê€": "r", 
    "s": "s", "á´›": "t", "á´œ": "u", "á´ ": "v", "á´¡": "w", "x": "x", "Ê": "y", "á´¢": "z"}
special_signs = { "â€¦": "...", "â‚‚": "2"}

mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ğŸ˜‰':'wink','ğŸ˜‚':'joy','ğŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}

# this from other kernel
symbols_to_isolate = '.,?!-;*"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\x96\x92â—Â£â™¥â¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûâ€ Î¼âœ’â¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼â¬…â„…Â»Ğ’Ğ°Ğ²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—â–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹â¡Â«Ï†â…“â€âœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑĞ¸Ê¿âœ¨ã€‚É‘\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜âœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼Ê•ÉÌ£Î”â‚€âœâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜ï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'
symbols_to_delete = '\nğŸ•\rğŸµğŸ˜‘\xa0\ue014\t\uf818\uf04a\xadğŸ˜¢ğŸ¶ï¸\uf0e0ğŸ˜œğŸ˜ğŸ‘Š\u200b\u200eğŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ğŸ˜ğŸ’–ğŸ’µĞ•ğŸ‘ğŸ˜€ğŸ˜‚\u202a\u202cğŸ”¥ğŸ˜„ğŸ»ğŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ğŸ˜‹ğŸ‘×©×œ×•××‘×™ğŸ˜±â€¼\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\u2009ğŸšŒá´µÍğŸŒŸğŸ˜ŠğŸ˜³ğŸ˜§ğŸ™€ğŸ˜ğŸ˜•\u200fğŸ‘ğŸ˜®ğŸ˜ƒğŸ˜˜××¢×›×—ğŸ’©ğŸ’¯â›½ğŸš„ğŸ¼à®œğŸ˜–á´ ğŸš²â€ğŸ˜ŸğŸ˜ˆğŸ’ªğŸ™ğŸ¯ğŸŒ¹ğŸ˜‡ğŸ’”ğŸ˜¡\x7fğŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ğŸ™„ï¼¨ğŸ˜ \ufeff\u2028ğŸ˜‰ğŸ˜¤â›ºğŸ™‚\u3000ØªØ­ÙƒØ³Ø©ğŸ‘®ğŸ’™ÙØ²Ø·ğŸ˜ğŸ¾ğŸ‰ğŸ˜\u2008ğŸ¾ğŸ˜…ğŸ˜­ğŸ‘»ğŸ˜¥ğŸ˜”ğŸ˜“ğŸ½ğŸ†ğŸ»ğŸ½ğŸ¶ğŸŒºğŸ¤”ğŸ˜ª\x08â€‘ğŸ°ğŸ‡ğŸ±ğŸ™†ğŸ˜¨ğŸ™ƒğŸ’•ğ˜Šğ˜¦ğ˜³ğ˜¢ğ˜µğ˜°ğ˜¤ğ˜ºğ˜´ğ˜ªğ˜§ğ˜®ğ˜£ğŸ’—ğŸ’šåœ°ç„è°·ÑƒĞ»ĞºĞ½ĞŸĞ¾ĞĞğŸ¾ğŸ•ğŸ˜†×”ğŸ”—ğŸš½æ­Œèˆä¼ğŸ™ˆğŸ˜´ğŸ¿ğŸ¤—ğŸ‡ºğŸ‡¸Ğ¼Ï…Ñ‚Ñ•â¤µğŸ†ğŸƒğŸ˜©\u200ağŸŒ ğŸŸğŸ’«ğŸ’°ğŸ’ÑĞ¿Ñ€Ğ´\x95ğŸ–ğŸ™…â›²ğŸ°ğŸ¤ğŸ‘†ğŸ™Œ\u2002ğŸ’›ğŸ™ğŸ‘€ğŸ™ŠğŸ™‰\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\x13ğŸš¬ğŸ¤“\ue602ğŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª××“×£× ×¨×š×¦×˜ğŸ˜’ÍğŸ†•ğŸ‘…ğŸ‘¥ğŸ‘„ğŸ”„ğŸ”¤ğŸ‘‰ğŸ‘¤ğŸ‘¶ğŸ‘²ğŸ”›ğŸ“\uf0b7\uf04c\x9f\x10æˆéƒ½ğŸ˜£âºğŸ˜ŒğŸ¤‘ğŸŒğŸ˜¯ĞµÑ…ğŸ˜²á¼¸á¾¶á½ğŸ’ğŸš“ğŸ””ğŸ“šğŸ€ğŸ‘\u202dğŸ’¤ğŸ‡\ue613å°åœŸè±†ğŸ¡â”â‰\u202fğŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ğŸ‡¹ğŸ‡¼ğŸŒ¸è”¡è‹±æ–‡ğŸŒğŸ²ãƒ¬ã‚¯ã‚µã‚¹ğŸ˜›å¤–å›½äººå…³ç³»Ğ¡Ğ±ğŸ’‹ğŸ’€ğŸ„ğŸ’œğŸ¤¢ÙÙÑŒÑ‹Ğ³Ñä¸æ˜¯\x9c\x9dğŸ—‘\u2005ğŸ’ƒğŸ“£ğŸ‘¿à¼¼ã¤à¼½ğŸ˜°á¸·Ğ—Ğ·â–±Ñ†ï¿¼ğŸ¤£å–æ¸©å“¥åè®®ä¼šä¸‹é™ä½ å¤±å»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨éª—å­ğŸãƒ„ğŸ…\x85ğŸºØ¢Ø¥Ø´Ø¡ğŸµğŸŒÍŸá¼”æ²¹åˆ«å…‹ğŸ¤¡ğŸ¤¥ğŸ˜¬ğŸ¤§Ğ¹\u2003ğŸš€ğŸ¤´Ê²ÑˆÑ‡Ğ˜ĞĞ Ğ¤Ğ”Ğ¯ĞœÑĞ¶ğŸ˜ğŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ğŸ’¨åœ†æ˜å›­×§â„ğŸˆğŸ˜ºğŸŒâá»‡ğŸ”ğŸ®ğŸğŸ†ğŸ‘ğŸŒ®ğŸŒ¯ğŸ¤¦\u200dğ“’ğ“²ğ“¿ğ“µì•ˆì˜í•˜ì„¸ìš”Ğ–Ñ™ĞšÑ›ğŸ€ğŸ˜«ğŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æğŸ¼ğŸ•ºğŸ¸ğŸ¥‚ğŸ—½ğŸ‡ğŸŠğŸ†˜ğŸ¤ ğŸ‘©ğŸ–’ğŸšªå¤©ä¸€å®¶âš²\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒğŸ‡«ğŸ‡·ğŸ‡©ğŸ‡ªğŸ‡®ğŸ‡¬ğŸ‡§ğŸ˜·ğŸ‡¨ğŸ‡¦Ğ¥Ğ¨ğŸŒ\x1fæ€é¸¡ç»™çŒ´çœ‹Êğ—ªğ—µğ—²ğ—»ğ˜†ğ—¼ğ˜‚ğ—¿ğ—®ğ—¹ğ—¶ğ˜‡ğ—¯ğ˜ğ—°ğ˜€ğ˜…ğ—½ğ˜„ğ—±ğŸ“ºÏ–\u2000Ò¯Õ½á´¦á¥Ò»Íº\u2007Õ°\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆğ“ğ¡ğğ«ğ®ğğšğƒğœğ©ğ­ğ¢ğ¨ğ§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ğ†á´‘Üğ¬ğ°ğ²ğ›ğ¦ğ¯ğ‘ğ™ğ£ğ‡ğ‚ğ˜ğŸÔœĞ¢á—à±¦ã€”á«ğ³ğ”ğ±ğŸ”ğŸ“ğ…ğŸ‹ï¬ƒğŸ’˜ğŸ’“Ñ‘ğ˜¥ğ˜¯ğ˜¶ğŸ’ğŸŒ‹ğŸŒ„ğŸŒ…ğ™¬ğ™–ğ™¨ğ™¤ğ™£ğ™¡ğ™®ğ™˜ğ™ ğ™šğ™™ğ™œğ™§ğ™¥ğ™©ğ™ªğ™—ğ™ğ™ğ™›ğŸ‘ºğŸ·â„‹ğ€ğ¥ğªğŸš¶ğ™¢á¼¹ğŸ¤˜Í¦ğŸ’¸Ø¬íŒ¨í‹°ï¼·ğ™‡áµ»ğŸ‘‚ğŸ‘ƒÉœğŸ«\uf0a7Ğ‘Ğ£Ñ–ğŸš¢ğŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ğŸƒğ“¬ğ“»ğ“´ğ“®ğ“½ğ“¼â˜˜ï´¾Ì¯ï´¿â‚½\ue807ğ‘»ğ’†ğ’ğ’•ğ’‰ğ’“ğ’–ğ’‚ğ’ğ’…ğ’”ğ’ğ’—ğ’ŠğŸ‘½ğŸ˜™\u200cĞ›â€’ğŸ¾ğŸ‘¹âŒğŸ’â›¸å…¬å¯“å…»å® ç‰©å—ğŸ„ğŸ€ğŸš‘ğŸ¤·æ“ç¾ğ’‘ğ’šğ’ğ‘´ğŸ¤™ğŸ’æ¬¢è¿æ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ğ™«ğŸˆğ’Œğ™Šğ™­ğ™†ğ™‹ğ™ğ˜¼ğ™…ï·»ğŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ğŸš—ğŸ³ğŸğŸğŸ–ğŸ‘ğŸ•ğ’„ğŸ—ğ ğ™„ğ™ƒğŸ‘‡é”Ÿæ–¤æ‹·ğ—¢ğŸ³ğŸ±ğŸ¬â¦ãƒãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ğ˜¿ğ™”â‚µğ’©â„¯ğ’¾ğ“ğ’¶ğ“‰ğ“‡ğ“Šğ“ƒğ“ˆğ“…â„´ğ’»ğ’½ğ“€ğ“Œğ’¸ğ“ğ™Î¶ğ™Ÿğ˜ƒğ—ºğŸ®ğŸ­ğŸ¯ğŸ²ğŸ‘‹ğŸ¦Šå¤šä¼¦ğŸ½ğŸ»ğŸ¹â›“ğŸ¹ğŸ·ğŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸å…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ğŸ¸ğŸ¤•ğŸ¤’â›‘ğŸæ‰¹åˆ¤æ£€è®¨ğŸğŸ¦ğŸ™‹ğŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ì˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ğŸ”«ğŸ‘å‡¸á½°ğŸ’²ğŸ—¯ğ™ˆá¼Œğ’‡ğ’ˆğ’˜ğ’ƒğ‘¬ğ‘¶ğ•¾ğ–™ğ–—ğ–†ğ–ğ–Œğ–ğ–•ğ–Šğ–”ğ–‘ğ–‰ğ–“ğ–ğ–œğ–ğ–šğ–‡ğ•¿ğ–˜ğ–„ğ–›ğ–’ğ–‹ğ–‚ğ•´ğ–Ÿğ–ˆğ•¸ğŸ‘‘ğŸš¿ğŸ’¡çŸ¥å½¼ç™¾\uf005ğ™€ğ’›ğ‘²ğ‘³ğ‘¾ğ’‹ğŸ’ğŸ˜¦ğ™’ğ˜¾ğ˜½ğŸğ˜©ğ˜¨á½¼á¹‘ğ‘±ğ‘¹ğ‘«ğ‘µğ‘ªğŸ‡°ğŸ‡µğŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘á“€á£ğŸ„ğŸˆğŸ”¨ğŸğŸ¤ğŸ¸ğŸ’ŸğŸ°ğŸŒğŸ›³ç‚¹å‡»æŸ¥ç‰ˆğŸ­ğ‘¥ğ‘¦ğ‘§ï¼®ï¼§ğŸ‘£\uf020ã£ğŸ‰Ñ„ğŸ’­ğŸ¥ÎğŸ´ğŸ‘¨ğŸ¤³ğŸ¦\x0bğŸ©ğ‘¯ğ’’ğŸ˜—ğŸğŸ‚ğŸ‘³ğŸ—ğŸ•‰ğŸ²Ú†ÛŒğ‘®ğ—•ğ—´ğŸ’êœ¥â²£â²ğŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ğŸ’Šã€Œã€\uf203\uf09a\uf222\ue608\uf202\uf099\uf469\ue607\uf410\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆĞ“ğ‘©ğ‘°ğ’€ğ‘ºğŸŒ¤ğ—³ğ—œğ—™ğ—¦ğ—§ğŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ğŸ‡³ğ’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ğ’ğŸ”¹ğŸ¤šğŸğ‘·ğŸ‚ğŸ’…ğ˜¬ğ˜±ğ˜¸ğ˜·ğ˜ğ˜­ğ˜“ğ˜–ğ˜¹ğ˜²ğ˜«Ú©Î’ÏğŸ’¢ÎœÎŸÎÎ‘Î•ğŸ‡±â™²ğˆâ†´ğŸ’’âŠ˜È»ğŸš´ğŸ–•ğŸ–¤ğŸ¥˜ğŸ“ğŸ‘ˆâ•ğŸš«ğŸ¨ğŸŒ‘ğŸ»ğğğŠğ‘­ğŸ¤–ğŸğŸ˜¼ğŸ•·ï½‡ï½’ï½ï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ğŸ°ğŸ‡´ğŸ‡­ğŸ‡»ğŸ‡²ğ—ğ—­ğ—˜ğ—¤ğŸ‘¼ğŸ“‰ğŸŸğŸ¦ğŸŒˆğŸ”­ã€ŠğŸŠğŸ\uf10aáƒšÚ¡ğŸ¦\U0001f92f\U0001f92ağŸ¡ğŸ’³á¼±ğŸ™‡ğ—¸ğ—Ÿğ— ğ—·ğŸ¥œã•ã‚ˆã†ãªã‚‰ğŸ”¼'

treetokenizer = TreebankWordTokenizer()

class preprocess_class(object):
    
    def __init__(self, treetokenizer, remove_dict, isolate_dict=None):
        
        self.tokenizer = treetokenizer
        self.isolate_dict = isolate_dict
        self.remove_dict  = remove_dict
        
    def bert_preprocess(self, x):
        
        x = self.correct_spelling(x)
        x = x.lower()
        x = self.clean_contractions(x)
        x = self.clean_special_chars(x, replace_punct=False)
        
        x = self.handle_punctuation(x)
        x = self.handle_contractions(x)
        x = self.fix_quote(x, normal=False)
        
        return x
    
    def normal_preprocess(self, x):
        
        x = self.clean_contractions(x)
        x = self.clean_special_chars(x)
        x = self.correct_spelling(x)
        
        x = self.handle_punctuation(x)
        x = self.handle_contractions(x)
        x = self.fix_quote(x)
        
        return x
    
    def correct_spelling(self, x):
        for word in mispell_dict.keys():
            x = x.replace(word, mispell_dict[word])
            
        return x
    
    def clean_contractions(self, text):
        specials = ["â€™", "â€˜", "Â´", "`"]
        for s in specials:
            text = text.replace(s, "'")
        text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(" ")])
        return text
    
    def clean_special_chars(self, text, replace_punct=True):
        for p in punct_mapping:
            text = text.replace(p, punct_mapping[p])
        if replace_punct:
            for p in punct:
                text = text.replace(p, f' {p} ')
        for s in specials:
            text = text.replace(s, specials[s])
        for s in small_caps_mapping:
            text = text.replace(s, small_caps_mapping[s])
        for s in special_signs:
            text = text.replace(s, special_signs[s])
        return text
    
    def handle_punctuation(self, x):
        x = x.translate(self.remove_dict)
        if self.isolate_dict is not None:
            x = x.translate(self.isolate_dict)
        return x
        
    def handle_contractions(self, x):
        return self.tokenizer.tokenize(x)
    
    def fix_quote(self, x, normal=True):
        
        if normal:
            x = [x_[1:] if x_.startswith("'") else x_ for x_ in x]
        else:
            x = [x_[1:] if (x_.startswith("'") and x_ != "'s") else x_ for x_ in x]
        x = ' '.join(x)
        
        return x
    
def convert_lines(example, max_seq_length,tokenizer, GPT=False):
    if GPT == False:
        max_seq_length -=2
    all_tokens = []
    longer = 0
    for text in example:
        tokens_a = tokenizer.tokenize(text)
        #print(tokens_a)
        if len(tokens_a)>max_seq_length:
            tokens_a = tokens_a[:max_seq_length]
            longer += 1
        if GPT:
            one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0]*(max_seq_length - len(tokens_a))
        else:
            one_token = tokenizer.convert_tokens_to_ids(["[CLS]"]+tokens_a+["[SEP]"])+[0] * (max_seq_length - len(tokens_a))
        all_tokens.append(one_token)
    return np.array(all_tokens)

class custom_output_layer(nn.Module):
    
    def __init__(self, input_dim):
        super(custom_output_layer, self).__init__()
        self.linear_out = nn.Linear(input_dim, 1)
        self.linear_aux_out = nn.Linear(input_dim, 6)
        
    def forward(self, x):
        
        result = self.linear_out(x)
        aux_result = self.linear_aux_out(x)
        out = torch.cat([result, aux_result], 1)
        
        return out

def bert_predict(test_df_bert, bert_model_path, trained_weight_path, num_labels, 
                 use_preprocess=True, use_custom_layer=False):
    
    bert_config = BertConfig( os.path.join(trained_weight_path, 'bert_config.json'))
    berttokenizer = BertTokenizer.from_pretrained(bert_model_path, cache_dir=None,do_lower_case=True)
    with open(config.BERT_VOCAB) as f:
        vocab_txt = f.read()
    vocab_txt = vocab_txt.split('\n')
    symbols = symbols_to_delete + symbols_to_isolate
    bert_not_to_delete = []
    bert_to_delete     = []
    for sym in symbols:
        if sym in vocab_txt:
            bert_not_to_delete.append(sym)
        elif ('##'+sym) in vocab_txt:
            bert_not_to_delete.append(sym)
        else:
            bert_to_delete.append(sym)
    remove_dict = {ord(c):f'' for c in bert_to_delete}
    
    if use_preprocess:
        print("preprocessing...")
        preprocessor = preprocess_class(treetokenizer, remove_dict)
        test_df['comment_text'] = test_df['comment_text'].apply(lambda x:preprocessor.bert_preprocess(x))
    test_df['comment_text'] = test_df['comment_text'].astype(str) 
    print("converting word to index...BERT")
    X_test = convert_lines(test_df["comment_text"].fillna("DUMMY_VALUE"), config.MAX_SEQUENCE_LENGTH, berttokenizer)
    
    if use_custom_layer:
        model = BertForSequenceClassification(bert_config, num_labels=1)
        in_features = model.classifier.in_features
        model.classifier = custom_output_layer(in_features)
    else:
        model = BertForSequenceClassification(bert_config, num_labels=num_labels)
        
    model.load_state_dict(torch.load(os.path.join(trained_weight_path, 'bert_pytorch.bin')))
    model.to(device)
    for param in model.parameters():
        param.requires_grad = False
    model.eval()
    test_preds = np.zeros((len(X_test)))
    test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))
    test_loader = torch.utils.data.DataLoader(test, batch_size=128, shuffle=False)
    tk0 = test_loader
    print("predict start (bert)...")
    for i, (x_batch,) in enumerate(tk0):
        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)
        test_preds[i * 128:(i + 1) * 128] = pred[:, 0].detach().cpu().squeeze().numpy()
    print("predict end (bert)...")    
    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()
    
    return test_pred

seed_everything(config.SEED)
test_df = pd.read_csv(config.CSV['test'])#.head(10)
start_time = time.time()
print("-----------------------------start bert predict----------------------------------")
BERT_predict = bert_predict(test_df, 
                            bert_model_path = config.BERT_MODEL_PATH['base'],
                            trained_weight_path = config.BERT_MODEL_WEIGHT['bert_1'],
                            num_labels = 8,
                            use_preprocess = False, 
                            use_custom_layer = False)


print("-------------------------------end bert predict----------------------------------")                               
print('BERT spend time:', time.time()-start_time)

print("reset module path...GPT")
sys.path.remove(package_dir)
ppbert_keys = [key for key in sys.modules.keys() if key.startswith('pytorch_pretrained')]
for key in ppbert_keys: sys.modules.pop(key)
os.system('pip install ../input/ppbert-pure/pytorch-pretrained-bert-pure/pytorch-pretrained-BERT-master/ --upgrade')
#sys.path.append(package_dir_a)
print("end reset module path...GPT")

print("reloading test dataset...GPT/LSTM")
print("preprocessing...GPT/LSTM")
isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}
remove_dict = {ord(c):f'' for c in symbols_to_delete}
preprocessor = preprocess_class(treetokenizer, remove_dict, isolate_dict)
test_df = pd.read_csv(config.CSV['test'])#.head(10)
test_df['comment_text'] = test_df['comment_text'].apply(lambda x:preprocessor.normal_preprocess(x))

print("-----------------------------start GPT2 predict----------------------------------")
start_time = time.time()
from pytorch_pretrained_bert.modeling_gpt2 import GPT2PreTrainedModel, GPT2Model
from pytorch_pretrained_bert import GPT2Tokenizer, GPT2Config, OpenAIAdam
print(sys.modules['pytorch_pretrained_bert'])




class GPT2ClassificationHeadModel(GPT2PreTrainedModel):

    def __init__(self, config, clf_dropout=0.4, n_class=8):
        super(GPT2ClassificationHeadModel, self).__init__(config)
        self.transformer = GPT2Model(config)
        self.dropout = nn.Dropout(clf_dropout)
        self.linear = nn.Linear(config.n_embd * 2, n_class)

        nn.init.normal_(self.linear.weight, std = 0.02)
        nn.init.normal_(self.linear.bias, 0)
        
        self.apply(self.init_weights)

    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):
        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)
        avg_pool = torch.mean(hidden_states, 1)
        max_pool, _ = torch.max(hidden_states, 1)
        h_conc = torch.cat((avg_pool, max_pool), 1)
        logits = self.linear(self.dropout(h_conc))
        return logits

Gptconfig = GPT2Config(config.GPT_MODEL_PATH +'config.json')
Gpttokenizer = GPT2Tokenizer.from_pretrained(config.GPT_MODEL_PATH)
model = GPT2ClassificationHeadModel(Gptconfig, clf_dropout=0.4, n_class=8)
model.load_state_dict(torch.load(config.GPT_MODEL_WEIGHT))
print("converting word to index...GPT")
X_test = convert_lines(test_df["comment_text"].fillna("DUMMY_VALUE"), 
                       config.MAX_SEQUENCE_LENGTH, 
                       Gpttokenizer, 
                       GPT=True)

model.to(device)
for param in model.parameters():
    param.requires_grad = False
model.eval()
batch_size = 64
test_preds = np.zeros((len(X_test)))
print("predict start (GPT2)...")
test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))
test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)
for i, (x_batch,) in enumerate(test_loader):
    pred = model(x_batch.to(device))
    test_preds[i * batch_size:(i + 1) * batch_size] = pred[:, 0].detach().cpu().squeeze().numpy()
GPT_predict = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()
print("predict end (GPT2)...")
print("-------------------------------end GPT2 predict----------------------------------")
print('GPT spend time:', time.time()-start_time)

print("-----------------------------start LSTM predict----------------------------------")
start_time = time.time()
import fastai
from fastai.train import Learner
from fastai.train import DataBunch
from fastai.callbacks import *
from fastai.basic_data import DatasetType
import fastprogress
from fastprogress import force_console_behavior
from pprint import pprint
from keras.preprocessing import text, sequence


with open(config.WORD_INDEX_PATH, 'rb') as f:
    word_index = pickle.load(f)

def load_embeddings(path):
    with open(path,'rb') as f:
        emb_arr = pickle.load(f)
    return emb_arr

def build_matrix(word_index, path):
    embedding_index = load_embeddings(path)
    embedding_matrix = np.zeros((config.MAX_FEATURES + 1, 300))
    unknown_words = []
    
    for word, i in word_index.items():
        if i <= config.MAX_FEATURES:
            try:
                embedding_matrix[i] = embedding_index[word]
            except KeyError:
                try:
                    embedding_matrix[i] = embedding_index[word.lower()]
                except KeyError:
                    try:
                        embedding_matrix[i] = embedding_index[word.title()]
                    except KeyError:
                        unknown_words.append(word)
    return embedding_matrix, unknown_words

class SpatialDropout(nn.Dropout2d):
    def forward(self, x):
        x = x.unsqueeze(2)    # (N, T, 1, K)
        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)
        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked
        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)
        x = x.squeeze(2)  # (N, T, K)
        return x

class NeuralNet(nn.Module):
    
    def __init__(self, embedding_matrix, num_aux_targets):
        super(NeuralNet, self).__init__()
        embed_size = embedding_matrix.shape[1]
        
        self.embedding = nn.Embedding(max_features, embed_size)
        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False
        self.embedding_dropout = SpatialDropout(0.3)
        
        self.lstm1 = nn.LSTM(embed_size, config.LSTM_UNITS, bidirectional=True, batch_first=True)
        self.lstm2 = nn.GRU(config.LSTM_UNITS * 2, config.LSTM_UNITS, bidirectional=True, batch_first=True)
    
        self.linear1 = nn.Linear(config.DENSE_HIDDEN_UNITS, config.DENSE_HIDDEN_UNITS)
        self.linear2 = nn.Linear(config.DENSE_HIDDEN_UNITS, config.DENSE_HIDDEN_UNITS)
        
        self.linear_out = nn.Linear(config.DENSE_HIDDEN_UNITS, 1)
        self.linear_aux_out = nn.Linear(config.DENSE_HIDDEN_UNITS, num_aux_targets)
        
    def forward(self, x, lengths=None):
        h_embedding = self.embedding(x.long())
        h_embedding = self.embedding_dropout(h_embedding)
        
        h_lstm1, _ = self.lstm1(h_embedding)
        h_lstm2, _ = self.lstm2(h_lstm1)
        
        # global average pooling
        avg_pool = torch.mean(h_lstm2, 1)
        # global max pooling
        max_pool, _ = torch.max(h_lstm2, 1)
        
        h_conc = torch.cat((max_pool, avg_pool), 1)
        h_conc_linear1  = F.relu(self.linear1(h_conc))
        h_conc_linear2  = F.relu(self.linear2(h_conc))
        
        hidden = h_conc + h_conc_linear1 + h_conc_linear2
        
        result = self.linear_out(hidden)
        aux_result = self.linear_aux_out(hidden)
        out = torch.cat([result, aux_result], 1)
        
        return out
        
    def save_my_weights(self, filename):
        my_weights = deepcopy(self.state_dict())
        my_weights.pop('embedding.weight')
        with open(filename, 'wb') as f:
            pickle.dump(my_weights, f)
        del my_weights
            
    def load_my_weights(self, filename):
        with open(filename, 'rb') as f:
            weights_dict = pickle.load(f)
        weights_dict['embedding.weight'] = self.embedding.weight
        self.load_state_dict(weights_dict)
        #print(self.state_dict())

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

class SequenceBucketCollator():
    def __init__(self, choose_length, sequence_index, length_index, label_index=None):
        self.choose_length = choose_length
        self.sequence_index = sequence_index
        self.length_index = length_index
        self.label_index = label_index
        
    def __call__(self, batch):
        batch = [torch.stack(x) for x in list(zip(*batch))]
        
        sequences = batch[self.sequence_index]
        lengths = batch[self.length_index]
        
        length = self.choose_length(lengths)
        mask = torch.arange(start=config.MAX_LSTM_LENGTH, end=0, step=-1) < length
        padded_sequences = sequences[:, mask]
        
        batch[self.sequence_index] = padded_sequences
        
        if self.label_index is not None:
            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]
    
        return batch

x_test = test_df['comment_text']
lstmtokenizer = text.Tokenizer(num_words = config.MAX_FEATURES, filters='',lower=False)
lstmtokenizer.word_index = word_index
crawl_matrix, unknown_words_crawl = build_matrix(lstmtokenizer.word_index, config.CRAWL_EMBEDDING_PATH)
print('n unknown words (crawl): ', len(unknown_words_crawl))

glove_matrix, unknown_words_glove = build_matrix(lstmtokenizer.word_index, config.GLOVE_EMBEDDING_PATH)
print('n unknown words (glove): ', len(unknown_words_glove))

max_features = config.MAX_FEATURES or len(lstmtokenizer.word_index) + 1

embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)
print('n embedding matrix shape: ', embedding_matrix.shape)

x_test = lstmtokenizer.texts_to_sequences(x_test)

del crawl_matrix
del glove_matrix
gc.collect()

test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))
x_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=config.MAX_LSTM_LENGTH))
batch_size = 512
test_dataset = torch.utils.data.TensorDataset(x_test_padded, test_lengths)

def predict_from_model(model_path,test,output_dim,
                       batch_size=512, 
                       n_epochs=5,
                       enable_checkpoint_ensemble=True):
    
    all_test_preds = []
    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]
    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)

    for epoch in range(n_epochs):
        test_preds = np.zeros((len(test), output_dim))
        model = NeuralNet(embedding_matrix, 6)
        model.to(device)
        model_name = model_path + 'model_' + str(model_idx) + '_epoch_' + str(epoch) + '.bin'
        model.load_my_weights(model_name)
        for param in model.parameters(): 
            param.requires_grad = False
        model.eval()
        print('loaded model of', model_name)
        
        for i, x_batch in enumerate(test_loader):
            X = x_batch[0].cuda()
            y_pred = sigmoid(model(X).detach().cpu().numpy())
            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred
            
        all_test_preds.append(test_preds)

    if enable_checkpoint_ensemble:
        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    
    else:
        test_preds = all_test_preds[-1]
        
    return test_preds


all_test_preds = []
for model_idx in range(config.NUM_MODELS):
    print('Model ', model_idx)
    seed_everything(1 + model_idx)
    test_preds = predict_from_model(config.RNN_PATH,test_dataset,output_dim=7) #, test_collator=test_collator)  
    all_test_preds.append(test_preds)

LSTM_predict = np.mean(all_test_preds, axis=0)[:, 0]
print("-----------------------------end LSTM predict----------------------------------")
print('LSTM spend time:', time.time()-start_time)


#-----------------------------BLEND----------------------------------
#from sklearn.metrics import mean_squared_error
#import math
#submission1 = pd.read_csv("../input/blend-test/submission_GPT.csv")
#submission2 = pd.read_csv("../input/blend-test/submission_BERT.csv")
#submission3 = pd.read_csv("../input/best-rnn//submission.csv")

#error1 = math.sqrt(mean_squared_error(submission2.prediction, BERT_predict))
#print('Bert Inference error:', error1)

#error2 = math.sqrt(mean_squared_error(submission1.prediction, GPT_predict))
#print('GPT Inference error:', error2)

#error3 = math.sqrt(mean_squared_error(submission3.prediction, LSTM_predict))
#print('LSTM Inference error:', error3)

FINAL_ANSWER = 0.7 * (0.2*LSTM_predict + 0.8*BERT_predict) + 0.3 * GPT_predict
submission = pd.DataFrame.from_dict({
    'id': test_df['id'],
    'prediction': FINAL_ANSWER})

submission.to_csv('submission.csv', index=False)
