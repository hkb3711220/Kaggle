{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sam                        cv:0.895 lb:0.893\n",
    "#use loss_kd_regularization cv:      lb:\n",
    "#use ema\n",
    "#add 2019 traindata         cv:      lb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "import collections\n",
    "import warnings\n",
    "import timm\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler, Sampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bi_tempered_loss_pytorch import bi_tempered_logistic_loss\n",
    "from aug_mix import RandomAugMix\n",
    "from sam.sam import SAM\n",
    "from FMix.fmix import sample_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "\n",
    "#     fold_num = 0\n",
    "    n_class = 5\n",
    "    train_path = \"./input/train_images/\"\n",
    "    train_2019_path = \"./input_2019/train/train/\"\n",
    "    loss_f = \"loss_kd_regularization\"\n",
    "    label_smoothing = 0.2\n",
    "    gamma = 2.0\n",
    "    num_workers = 4\n",
    "    resample = False\n",
    "    n_epochs = 40\n",
    "    \n",
    "    use_sam = True\n",
    "    use_ema = True #use model ema\n",
    "    use_mix_precision = False\n",
    "    use_mixup = False\n",
    "    use_cutmix = False\n",
    "    use_fmix = True\n",
    "    \n",
    "    accumulate_steps = 1\n",
    "    train_batch_size = 12\n",
    "    test_batch_size = 32\n",
    "    early_stop_patience = 5\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-6\n",
    "    seed = 2020\n",
    "    n_fold = 5\n",
    "    image_size = 512\n",
    "    folder = 'effnet-b4-ema'\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = True #True  # do scheduler.step after validation stage loss\n",
    "\n",
    "    SchedulerClass = lr_scheduler.CosineAnnealingLR\n",
    "    scheduler_params = dict(\n",
    "        T_max=n_epochs\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_kd_regularization(outputs, labels):\n",
    "    \"\"\"\n",
    "    loss function for mannually-designed regularization: Tf-KD_{reg}\n",
    "    \"\"\"\n",
    "    alpha = 0.1\n",
    "    T = 20\n",
    "    correct_prob = 0.99    # the probability for correct class in u(k)\n",
    "    loss_CE = F.cross_entropy(outputs, labels)\n",
    "    K = outputs.size(1)\n",
    "\n",
    "    teacher_soft = torch.ones_like(outputs).cuda()\n",
    "    teacher_soft = teacher_soft*(1-correct_prob)/(K-1)  # p^d(k)\n",
    "    for i in range(outputs.shape[0]):\n",
    "        teacher_soft[i ,labels[i]] = correct_prob\n",
    "    loss_soft_regu = nn.KLDivLoss()(F.log_softmax(outputs, dim=1), F.softmax(teacher_soft/T, dim=1))*50\n",
    "\n",
    "    KD_loss = (1. - alpha)*loss_CE + alpha*loss_soft_regu\n",
    "\n",
    "    return KD_loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    NLL loss with label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the LabelSmoothing module.\n",
    "        :param smoothing: label smoothing factor\n",
    "        \"\"\"\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        assert smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1. - smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2.0, eps=1e-7, reduction='mean'):\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        probs = torch.sigmoid(input)\n",
    "        log_probs = -torch.log(probs)\n",
    "        loss = torch.sum(torch.pow(1 - probs + self.eps, self.gamma).mul(log_probs).mul(target), dim=1)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, eps=1e-7, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        y = F.one_hot(target, num_classes=input.size(-1)).to(input.device)\n",
    "\n",
    "        logit = F.softmax(input, dim=-1)\n",
    "        logit = logit.clamp(self.eps, 1. - self.eps)\n",
    "\n",
    "        loss = -1 * y * torch.log(logit)  # cross entropy\n",
    "        loss = loss * (1 - logit) ** self.gamma  # focal loss\n",
    "        # loss = torch.sum(loss, dim=1)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "def focal_loss(labels, logits, alpha, gamma):\n",
    "    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n",
    "    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n",
    "    where pt is the probability of being classified to the true class.\n",
    "    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n",
    "    Args:\n",
    "        labels: A float tensor of size [batch, num_classes].\n",
    "        logits: A float tensor of size [batch, num_classes].\n",
    "        alpha: A float tensor of size [batch_size]\n",
    "            specifying per-example weight for balanced cross entropy.\n",
    "        gamma: A float scalar modulating loss from hard and easy examples.\n",
    "    Returns:\n",
    "        focal_loss: A float32 scalar representing normalized total loss.\n",
    "     \"\"\"\n",
    "    BCLoss = F.binary_cross_entropy_with_logits(input=logits, target=labels, reduction=\"none\")\n",
    "\n",
    "    if gamma == 0.0:\n",
    "        modulator = 1.0\n",
    "    else:\n",
    "        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 +\n",
    "                                                                               torch.exp(-1.0 * logits)))\n",
    "\n",
    "    loss = modulator * BCLoss\n",
    "\n",
    "    weighted_loss = alpha * loss\n",
    "    focal_loss = torch.sum(weighted_loss)\n",
    "\n",
    "    focal_loss /= torch.sum(labels)\n",
    "    return focal_loss\n",
    "\n",
    "class CB_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, samples_per_cls, n_class=5, loss_type=\"sigmoid\", beta=0.9999, gamma=2.0):\n",
    "\n",
    "        effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "        weights = (1.0 - beta) / np.array(effective_num)\n",
    "        self.weights = weights / np.sum(weights) * n_class\n",
    "\n",
    "\n",
    "        self.n_class = n_class\n",
    "        self.loss_type = loss_type\n",
    "        self.gamma =gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        weights = self.weights.copy()\n",
    "\n",
    "        weights = torch.tensor(weights).float()\n",
    "        weights = weights.unsqueeze(0)\n",
    "        weights = weights.repeat(target.shape[0], 1) * target\n",
    "        weights = weights.sum(1)\n",
    "        weights = weights.unsqueeze(1)\n",
    "        weights = weights.repeat(1, self.n_class)\n",
    "\n",
    "        if self.loss_type == \"focal\":\n",
    "            cb_loss = focal_loss(target, input, weights, self.gamma)\n",
    "        elif self.loss_type == \"sigmoid\":\n",
    "            cb_loss = F.binary_cross_entropy_with_logits(input=input, target=target, weights=weights)\n",
    "        elif self.loss_type == \"softmax\":\n",
    "            pred = input.softmax(dim=1)\n",
    "            cb_loss = F.binary_cross_entropy(input=pred, target=target, weight=weights)\n",
    "        return cb_loss\n",
    "\n",
    "def loss_function(config):\n",
    "    if config.loss_f == \"cross_entropy\":\n",
    "        return nn.CrossEntropyLoss()\n",
    "    if config.loss_f == \"focal\":\n",
    "        return FocalLoss(gamma=config.gamma)\n",
    "    if config.loss_f == \"binary_cross_entropy\":\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "    if config.loss_f == \"binary_focal_loss\":\n",
    "        return BinaryFocalLoss(gamma=config.gamma)\n",
    "    if config.loss_f == \"cb_loss\":\n",
    "        return CB_loss(config.sample_per_class)\n",
    "    if config.loss_f == \"labelsmoothingcrossentropy\":\n",
    "        return LabelSmoothingCrossEntropy(config.label_smoothing)\n",
    "    if config.loss_f == \"bi-tempered\":\n",
    "        return bi_tempered_logistic_loss\n",
    "    if config.loss_f == \"loss_kd_regularization\":\n",
    "        return loss_kd_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(image_size):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Transpose(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.5),\n",
    "#             A.ShiftScaleRotate(\n",
    "#                 shift_limit=(-0.2, 0.2), \n",
    "#                 scale_limit=(-0.2, 0.2), \n",
    "#                 rotate_limit=(-20, 20), \n",
    "#                 interpolation=cv2.INTER_LINEAR, \n",
    "#                 border_mode=cv2.BORDER_REFLECT_101,\n",
    "#                 p=0.8),\n",
    "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            A.CoarseDropout(\n",
    "                p=0.5,\n",
    "                max_holes=100,\n",
    "                max_height=50,\n",
    "                max_width=50,\n",
    "                min_holes=30,\n",
    "                min_height=20,\n",
    "                min_width=20,\n",
    "            ),\n",
    "            A.Resize(height=image_size, width=image_size, p=1.0),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms(image_size):\n",
    "    return A.Compose(\n",
    "        [   \n",
    "#             A.RandomRotate90(p=0.5),\n",
    "#             A.Transpose(p=0.5),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=image_size, width=image_size, p=1.0),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_path(img_name, cfg):\n",
    "    \n",
    "    img_name = str(img_name)\n",
    "    if isfile(cfg.train_path + img_name):\n",
    "        return cfg.train_path + img_name\n",
    "    if isfile(cfg.train_2019_path + \"cbb/\" + img_name):\n",
    "        return cfg.train_2019_path + \"cbb/\" + img_name\n",
    "    if isfile(cfg.train_2019_path + \"cbsd/\" + img_name):\n",
    "        return cfg.train_2019_path + \"cbsd/\" + img_name\n",
    "    if isfile(cfg.train_2019_path + \"cgm/\" + img_name):\n",
    "        return cfg.train_2019_path + \"cgm/\" + img_name\n",
    "    if isfile(cfg.train_2019_path + \"cmd/\" + img_name):\n",
    "        return cfg.train_2019_path + \"cmd/\" + img_name\n",
    "    if isfile(cfg.train_2019_path + \"healthy/\" + img_name):\n",
    "        return cfg.train_2019_path + \"healthy/\" + img_name\n",
    "\n",
    "    return img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cassava_dataset(Dataset):\n",
    "    def __init__(self, cfg, dataframe, transforms=None, mode=\"train\"):\n",
    "        self.config = cfg\n",
    "        self.image_size = cfg.image_size\n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.mixup = True\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index: int):\n",
    "        image, label = self.get_image_and_label(index)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)[\"image\"]\n",
    "        return image, label\n",
    "    def get_image_and_label(self, index):\n",
    "        data = self.df.iloc[index]\n",
    "        image_id = data[\"image_id\"]\n",
    "        image_path = expand_path(image_id, self.config)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)#.astype(np.float32)\n",
    "        label = data[\"label\"]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_data(x, y, alpha=1.0, use_cuda=True):\n",
    "\n",
    "    mixed_x = x.clone()\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    def rand_bbox(size, lam):\n",
    "        W = size[2]\n",
    "        H = size[3]\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = np.int(W * cut_rat)\n",
    "        cut_h = np.int(H * cut_rat)\n",
    "\n",
    "        # uniform\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "        return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        rand_index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        rand_index = torch.randperm(batch_size)\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(mixed_x.size(), lam)\n",
    "    mixed_x[:, :, bbx1:bbx2, bby1:bby2] = mixed_x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (mixed_x.size()[-1] * mixed_x.size()[-2]))\n",
    "    y_a, y_b = y, y[rand_index]\n",
    "\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmix_data(x, y, alpha=1.0, decay_power=3, size=(512, 512), max_soft=0.0, reformulate=False, use_cuda=True):\n",
    "    \n",
    "    lam, mask = sample_mask(alpha, decay_power, size, max_soft, reformulate)\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    mask = torch.from_numpy(mask).float().to(x.device)\n",
    "    \n",
    "    mixed_x = mask * x + (1 - mask) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_parallel(model):\n",
    "    # is model is parallel with DP or DDP\n",
    "    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\n",
    "\n",
    "class ModelEMA:\n",
    "    \"\"\" Model Exponential Moving Average from https://github.com/rwightman/pytorch-image-models\n",
    "    Keep a moving average of everything in the model state_dict (parameters and buffers).\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    A smoothed version of the weights is necessary for some training schemes to perform well.\n",
    "    This class is sensitive where it is initialized in the sequence of model init,\n",
    "    GPU assignment and distributed training wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, updates=0):\n",
    "        # Create EMA\n",
    "        self.ema = deepcopy(model.module if is_parallel(model) else model).eval()  # FP32 EMA\n",
    "        # if next(model.parameters()).device.type != 'cpu':\n",
    "        #     self.ema.half()  # FP16 EMA\n",
    "        self.updates = updates  # number of EMA updates\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # decay exponential ramp (to help early epochs)\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "\n",
    "            msd = model.module.state_dict() if is_parallel(model) else model.state_dict()  # model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1. - d) * msd[k].detach()\n",
    "\n",
    "    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):\n",
    "        # Update EMA attributes\n",
    "        copy_attr(self.ema, model, include, exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        \n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        self.early_stop_count = 0\n",
    "        self.fold_num = config.fold_num\n",
    "        self.base_dir = f'./{config.folder}'\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10 ** 5\n",
    "        self.best_score = 0.0\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        self.use_sam = config.use_sam\n",
    "        self.use_mix_precision = config.use_mix_precision\n",
    "        self.use_mixup = config.use_mixup\n",
    "        self.use_cutmix = config.use_cutmix\n",
    "        self.use_fmix = config.use_fmix\n",
    "        #ema\n",
    "        self.use_ema = config.use_ema\n",
    "        self.model_ema = None\n",
    "        self.model_ema_decay = 0.999\n",
    "        \n",
    "        self.accumulate_steps = config.accumulate_steps\n",
    "        self.valset_size = config.valset_size\n",
    "        self.test_batch_size = config.test_batch_size\n",
    "        self.loss_function_train = loss_function(config)\n",
    "        self.loss_function_test = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.early_stop_patience = config.early_stop_patience\n",
    "        \n",
    "        self.target_names = [\"cbb\", \"cbsd\", \"cgm\", \"cmd\", \"healthy\"]\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        if config.use_sam:\n",
    "            base_optimizer = torch.optim.Adam\n",
    "            self.optimizer = SAM(self.model.parameters(), base_optimizer, lr=config.lr, weight_decay=config.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        if self.use_mix_precision:\n",
    "            self.scaler = GradScaler()\n",
    "        if self.use_ema:\n",
    "            self.model_ema = ModelEMA(self.model, decay=self.model_ema_decay)\n",
    "            \n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}. fold num is {self.fold_num}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        \n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "            t = time.time()\n",
    "            summary_loss = self.train_one_epoch(train_loader)\n",
    "            self.log(\n",
    "                f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "            t = time.time()\n",
    "            \n",
    "            if self.use_ema:\n",
    "                summary_loss, score, report = self.validation(self.model_ema.ema.module if hasattr(self.model_ema.ema, 'module') else self.model_ema.ema, \n",
    "                                               validation_loader)\n",
    "            else:\n",
    "                summary_loss, score, report = self.validation(self.model, validation_loader)\n",
    "            \n",
    "            self.log(\n",
    "                f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.log(f'[RESULT]: Val. Score: {score:.5f}')\n",
    "            self.log(f'[RESULT]: Val. Report: \\n{report}')\n",
    "            \n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/{self.fold_num}-best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/{self.fold_num}-best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "                    \n",
    "            if score > self.best_score:\n",
    "                self.best_score = score\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/{self.fold_num}-best-score-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/{self.fold_num}-best-score-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "                self.early_stop_count = 0\n",
    "            else:\n",
    "                self.early_stop_count += 1\n",
    "            \n",
    "            if self.early_stop_count == self.early_stop_patience:\n",
    "                break\n",
    "            #break\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step()\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, model, val_loader):\n",
    "        \n",
    "        model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        predicts = np.zeros(self.valset_size)\n",
    "        truths = np.zeros(self.valset_size)\n",
    "        for step, (images, labels) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            start = step * self.test_batch_size\n",
    "            end = min(start + self.test_batch_size, self.valset_size)\n",
    "            with torch.no_grad():\n",
    "                images = images.to(self.device).float()\n",
    "                targets = labels.long().to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_function_test(outputs, targets)\n",
    "                summary_loss.update(loss.detach().item())\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                predicts[start:end] = np.argmax(probs.detach().cpu().numpy(), axis=1).flatten()\n",
    "                truths[start:end] = labels.detach().cpu().numpy().flatten()\n",
    "            #break\n",
    "            \n",
    "        score = accuracy_score(truths, predicts)\n",
    "        report = classification_report(truths, predicts, target_names=self.target_names)\n",
    "        \n",
    "        return summary_loss, score, report\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        \n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        self.optimizer.zero_grad()  # very important\n",
    "        for step, (images, labels) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            images = images.float().to(self.device)\n",
    "            targets = labels.long().to(self.device)\n",
    "            original_images = True\n",
    "\n",
    "            if self.use_mixup and random.random() < 0.5:\n",
    "                images, targets_a, targets_b, lam = mixup_data(images, targets, alpha=1.0, use_cuda=True)\n",
    "                original_images = False\n",
    "            \n",
    "            if self.use_cutmix and original_images and random.random() < 0.5:\n",
    "                images, targets_a, targets_b, lam = cutmix_data(images, targets)\n",
    "                original_images = False\n",
    "                \n",
    "            if self.use_fmix and original_images and random.random() < 0.5:\n",
    "                images, targets_a, targets_b, lam = fmix_data(images, targets)\n",
    "                original_images = False\n",
    "                \n",
    "            if original_images:\n",
    "                targets_a = targets\n",
    "                targets_b = targets\n",
    "                lam = 1.0\n",
    "\n",
    "            if self.use_mix_precision:\n",
    "                assert self.use_sam == False\n",
    "                with autocast():\n",
    "                    outputs = self.model(images)\n",
    "                    loss = mixup_criterion(self.loss_function_train, outputs, targets_a, targets_b, lam)\n",
    "                    loss /= self.accumulate_steps\n",
    "                self.scaler.scale(loss).backward()\n",
    "                if (step + 1) % self.accumulate_steps == 0:  # Wait for several backward steps\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "            elif self.use_sam:\n",
    "                loss = mixup_criterion(self.loss_function_train, self.model(images), targets_a, targets_b, lam)\n",
    "                loss.backward()\n",
    "                self.optimizer.first_step(zero_grad=True)\n",
    "                    \n",
    "                loss = mixup_criterion(self.loss_function_train, self.model(images), targets_a, targets_b, lam)\n",
    "                loss.backward()\n",
    "                self.optimizer.second_step(zero_grad=True)\n",
    "            else:\n",
    "                assert self.use_sam == False\n",
    "                outputs = self.model(images)\n",
    "                loss = mixup_criterion(self.loss_function_train, outputs, targets_a, targets_b, lam)\n",
    "                loss /= self.accumulate_steps\n",
    "                loss.backward()\n",
    "                if (step + 1) % self.accumulate_steps == 0:  # Wait for several backward steps\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.model_ema is not None:\n",
    "                self.model_ema.update(self.model)\n",
    "            \n",
    "            summary_loss.update(loss.detach().item() * self.accumulate_steps)\n",
    "            if self.config.step_scheduler and (step + 1) % self.accumulate_steps == 0:\n",
    "                self.scheduler.step()\n",
    "            #break\n",
    "                    \n",
    "        return summary_loss\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        \n",
    "        if self.use_ema:\n",
    "            torch.save({\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'state_dict_ema': self.model_ema.ema.module.state_dict() if hasattr(self.model_ema.ema, 'module') else self.model_ema.ema.state_dict(),\n",
    "            }, path)\n",
    "        else:\n",
    "            torch.save({\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "            }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        return dataset.df.iloc[idx][\"label\"]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleSampler(Sampler):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.classes_num = len(np.unique(dataset.df[\"label\"].values))\n",
    "        label_to_count = [0] * self.classes_num\n",
    "        indexes_per_class = []\n",
    "        self.indexes_per_class = [[],[],[],[],[]]\n",
    "        print(self.indexes_per_class)\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            label_to_count[label] += 1\n",
    "            self.indexes_per_class[label].append(idx)\n",
    "            \n",
    "        self.min_mun = min(label_to_count) \n",
    "        self.length = 0\n",
    "        for k in range(self.classes_num):\n",
    "            if len(self.indexes_per_class[k]) >= self.min_mun*3:\n",
    "                self.length += len(self.indexes_per_class[k])\n",
    "            else:\n",
    "                self.length += self.min_mun*2\n",
    "        print(self.length)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        all_indexs = []\n",
    "        \n",
    "        for k in range(self.classes_num):\n",
    "            if len(self.indexes_per_class[k]) >= self.min_mun*2:\n",
    "                all_indexs.extend(self.indexes_per_class[k])\n",
    "            else:\n",
    "                gap = self.min_mun*2 - len(self.indexes_per_class[k])\n",
    "                random_choice = np.random.choice(self.indexes_per_class[k], int(gap), replace=True)\n",
    "                all_indexs.extend(list(random_choice) + list(self.indexes_per_class[k]))\n",
    "        \n",
    "        l = np.array(all_indexs)\n",
    "        l = l.reshape(-1)\n",
    "        random.shuffle(l)\n",
    "        return iter(l)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.length)\n",
    "    \n",
    "    def _get_label(self, dataset, idx):\n",
    "        return dataset.df.iloc[idx][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(net, train_dataset, validation_dataset, config):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.train_batch_size,\n",
    "        sampler=RandomSampler(train_dataset) if config.resample == False else UpsampleSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=config.num_workers,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=config.test_batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=config)\n",
    "    fitter.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(in_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter prepared. Device is cuda. fold num is 0\n",
      "\n",
      "2021-01-08T01:18:35.118175\n",
      "LR: 0.0001\n",
      "[RESULT]: Train. Epoch: 0, summary_loss: 1.10148, time: 1700.73007\n",
      "[RESULT]: Val. Epoch: 0, summary_loss: 0.84572, time: 52.79914\n",
      "[RESULT]: Val. Score: 0.87360\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.51      0.65      0.57       218\n",
      "        cbsd       0.90      0.68      0.77       438\n",
      "         cgm       0.85      0.73      0.79       477\n",
      "         cmd       0.95      0.98      0.96      2631\n",
      "     healthy       0.69      0.75      0.72       516\n",
      "\n",
      "    accuracy                           0.87      4280\n",
      "   macro avg       0.78      0.76      0.76      4280\n",
      "weighted avg       0.88      0.87      0.87      4280\n",
      "\n",
      "\n",
      "2021-01-08T01:47:52.089972\n",
      "LR: 9.98458666866564e-05\n",
      "[RESULT]: Train. Epoch: 1, summary_loss: 1.03389, time: 1700.28576\n",
      "[RESULT]: Val. Epoch: 1, summary_loss: 0.78321, time: 52.31196\n",
      "[RESULT]: Val. Score: 0.88481\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.69      0.42      0.52       218\n",
      "        cbsd       0.85      0.79      0.82       438\n",
      "         cgm       0.83      0.80      0.82       477\n",
      "         cmd       0.95      0.98      0.96      2631\n",
      "     healthy       0.69      0.76      0.72       516\n",
      "\n",
      "    accuracy                           0.88      4280\n",
      "   macro avg       0.80      0.75      0.77      4280\n",
      "weighted avg       0.88      0.88      0.88      4280\n",
      "\n",
      "\n",
      "2021-01-08T02:17:06.378890\n",
      "LR: 9.938441702975689e-05\n",
      "[RESULT]: Train. Epoch: 2, summary_loss: 1.01692, time: 1701.56507\n",
      "[RESULT]: Val. Epoch: 2, summary_loss: 0.78518, time: 52.25157\n",
      "[RESULT]: Val. Score: 0.88785\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.66      0.58      0.62       218\n",
      "        cbsd       0.86      0.78      0.82       438\n",
      "         cgm       0.81      0.81      0.81       477\n",
      "         cmd       0.96      0.97      0.96      2631\n",
      "     healthy       0.71      0.77      0.74       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.80      0.78      0.79      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n",
      "\n",
      "2021-01-08T02:46:21.629471\n",
      "LR: 9.861849601988383e-05\n",
      "[RESULT]: Train. Epoch: 3, summary_loss: 1.00160, time: 1700.81290\n",
      "[RESULT]: Val. Epoch: 3, summary_loss: 0.78676, time: 52.25990\n",
      "[RESULT]: Val. Score: 0.88201\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.67      0.54      0.60       218\n",
      "        cbsd       0.84      0.78      0.80       438\n",
      "         cgm       0.84      0.75      0.80       477\n",
      "         cmd       0.97      0.96      0.96      2631\n",
      "     healthy       0.66      0.84      0.74       516\n",
      "\n",
      "    accuracy                           0.88      4280\n",
      "   macro avg       0.80      0.77      0.78      4280\n",
      "weighted avg       0.89      0.88      0.88      4280\n",
      "\n",
      "\n",
      "2021-01-08T03:15:35.833253\n",
      "LR: 9.755282581475767e-05\n",
      "[RESULT]: Train. Epoch: 4, summary_loss: 0.99867, time: 1702.42031\n",
      "[RESULT]: Val. Epoch: 4, summary_loss: 0.76058, time: 52.35235\n",
      "[RESULT]: Val. Score: 0.88598\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.68      0.50      0.58       218\n",
      "        cbsd       0.86      0.76      0.81       438\n",
      "         cgm       0.84      0.79      0.81       477\n",
      "         cmd       0.96      0.97      0.96      2631\n",
      "     healthy       0.68      0.83      0.74       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.80      0.77      0.78      4280\n",
      "weighted avg       0.89      0.89      0.88      4280\n",
      "\n",
      "\n",
      "2021-01-08T03:44:52.043974\n",
      "LR: 9.619397662556432e-05\n",
      "[RESULT]: Train. Epoch: 5, summary_loss: 0.98559, time: 1699.62338\n",
      "[RESULT]: Val. Epoch: 5, summary_loss: 0.76141, time: 52.32449\n",
      "[RESULT]: Val. Score: 0.88879\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.69      0.50      0.58       218\n",
      "        cbsd       0.89      0.77      0.83       438\n",
      "         cgm       0.77      0.85      0.81       477\n",
      "         cmd       0.96      0.97      0.96      2631\n",
      "     healthy       0.72      0.78      0.75       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.81      0.77      0.79      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n",
      "\n",
      "2021-01-08T04:14:05.495508\n",
      "LR: 9.455032620941837e-05\n",
      "[RESULT]: Train. Epoch: 6, summary_loss: 0.98211, time: 1703.23238\n",
      "[RESULT]: Val. Epoch: 6, summary_loss: 0.78430, time: 52.31800\n",
      "[RESULT]: Val. Score: 0.88201\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.55      0.78      0.65       218\n",
      "        cbsd       0.93      0.66      0.77       438\n",
      "         cgm       0.76      0.87      0.81       477\n",
      "         cmd       0.96      0.96      0.96      2631\n",
      "     healthy       0.76      0.71      0.73       516\n",
      "\n",
      "    accuracy                           0.88      4280\n",
      "   macro avg       0.79      0.80      0.79      4280\n",
      "weighted avg       0.89      0.88      0.88      4280\n",
      "\n",
      "\n",
      "2021-01-08T04:43:22.811110\n",
      "LR: 9.26320082177046e-05\n",
      "[RESULT]: Train. Epoch: 7, summary_loss: 0.97598, time: 1701.33418\n",
      "[RESULT]: Val. Epoch: 7, summary_loss: 0.74417, time: 52.32857\n",
      "[RESULT]: Val. Score: 0.89089\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.64      0.59      0.61       218\n",
      "        cbsd       0.87      0.79      0.83       438\n",
      "         cgm       0.80      0.83      0.81       477\n",
      "         cmd       0.96      0.97      0.97      2631\n",
      "     healthy       0.75      0.74      0.75       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.80      0.79      0.79      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n",
      "\n",
      "2021-01-08T05:12:38.189699\n",
      "LR: 9.045084971874737e-05\n",
      "[RESULT]: Train. Epoch: 8, summary_loss: 0.96945, time: 1701.22890\n",
      "[RESULT]: Val. Epoch: 8, summary_loss: 0.73333, time: 52.33720\n",
      "[RESULT]: Val. Score: 0.89439\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.69      0.59      0.64       218\n",
      "        cbsd       0.90      0.77      0.83       438\n",
      "         cgm       0.81      0.83      0.82       477\n",
      "         cmd       0.96      0.97      0.97      2631\n",
      "     healthy       0.73      0.79      0.76       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.82      0.79      0.80      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n",
      "\n",
      "2021-01-08T05:41:53.466687\n",
      "LR: 8.802029828000154e-05\n",
      "[RESULT]: Train. Epoch: 9, summary_loss: 0.96339, time: 1701.85583\n",
      "[RESULT]: Val. Epoch: 9, summary_loss: 0.75615, time: 52.29578\n",
      "[RESULT]: Val. Score: 0.88995\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.67      0.61      0.64       218\n",
      "        cbsd       0.91      0.77      0.83       438\n",
      "         cgm       0.81      0.81      0.81       477\n",
      "         cmd       0.96      0.97      0.96      2631\n",
      "     healthy       0.71      0.79      0.75       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.81      0.79      0.80      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n",
      "\n",
      "2021-01-08T06:11:08.752425\n",
      "LR: 8.535533905932737e-05\n",
      "[RESULT]: Train. Epoch: 10, summary_loss: 0.95700, time: 1702.63787\n",
      "[RESULT]: Val. Epoch: 10, summary_loss: 0.72893, time: 52.32605\n",
      "[RESULT]: Val. Score: 0.89299\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.70      0.54      0.61       218\n",
      "        cbsd       0.83      0.84      0.83       438\n",
      "         cgm       0.80      0.83      0.82       477\n",
      "         cmd       0.96      0.97      0.97      2631\n",
      "     healthy       0.76      0.74      0.75       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.81      0.78      0.80      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n",
      "\n",
      "2021-01-08T06:40:25.138594\n",
      "LR: 8.247240241650918e-05\n",
      "[RESULT]: Train. Epoch: 11, summary_loss: 0.95193, time: 1701.66141\n",
      "[RESULT]: Val. Epoch: 11, summary_loss: 0.73383, time: 52.34711\n",
      "[RESULT]: Val. Score: 0.88364\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.71      0.46      0.56       218\n",
      "        cbsd       0.84      0.77      0.81       438\n",
      "         cgm       0.80      0.82      0.81       477\n",
      "         cmd       0.96      0.96      0.96      2631\n",
      "     healthy       0.68      0.81      0.74       516\n",
      "\n",
      "    accuracy                           0.88      4280\n",
      "   macro avg       0.80      0.77      0.78      4280\n",
      "weighted avg       0.89      0.88      0.88      4280\n",
      "\n",
      "\n",
      "2021-01-08T07:09:40.273653\n",
      "LR: 7.938926261462365e-05\n",
      "[RESULT]: Train. Epoch: 12, summary_loss: 0.94705, time: 1700.51005\n",
      "[RESULT]: Val. Epoch: 12, summary_loss: 0.74280, time: 52.34031\n",
      "[RESULT]: Val. Score: 0.88972\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.67      0.66      0.67       218\n",
      "        cbsd       0.83      0.82      0.82       438\n",
      "         cgm       0.79      0.85      0.82       477\n",
      "         cmd       0.96      0.96      0.96      2631\n",
      "     healthy       0.75      0.72      0.73       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.80      0.80      0.80      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n",
      "\n",
      "2021-01-08T07:38:54.266991\n",
      "LR: 7.612492823579742e-05\n",
      "[RESULT]: Train. Epoch: 13, summary_loss: 0.93886, time: 1700.05346\n",
      "[RESULT]: Val. Epoch: 13, summary_loss: 0.73285, time: 52.37508\n",
      "[RESULT]: Val. Score: 0.88668\n",
      "[RESULT]: Val. Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.67      0.62      0.64       218\n",
      "        cbsd       0.90      0.74      0.81       438\n",
      "         cgm       0.77      0.86      0.81       477\n",
      "         cmd       0.97      0.96      0.96      2631\n",
      "     healthy       0.69      0.80      0.74       516\n",
      "\n",
      "    accuracy                           0.89      4280\n",
      "   macro avg       0.80      0.79      0.79      4280\n",
      "weighted avg       0.89      0.89      0.89      4280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    seed_everything(TrainGlobalConfig.seed)\n",
    "    train_csv = pd.read_csv(\"./input/train.csv\")\n",
    "#     train_2019_csv = pd.read_csv(\"./input_2019/train.csv\")\n",
    "    kfolds = StratifiedKFold(n_splits=TrainGlobalConfig.n_fold,\n",
    "                             random_state=TrainGlobalConfig.seed,\n",
    "                             shuffle=True).split(np.arange(train_csv.shape[0]), train_csv.label.values)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfolds):\n",
    "        if fold == 0: continue\n",
    "        TrainGlobalConfig.fold_num = fold\n",
    "        \n",
    "        net = CassvaImgClassifier(model_arch=\"tf_efficientnet_b4_ns\",\n",
    "                                  n_class=TrainGlobalConfig.n_class, \n",
    "                                  pretrained=True)\n",
    "        train_df = train_csv.loc[train_idx, :].reset_index(drop=True)\n",
    "        valid_df = train_csv.loc[val_idx, :].reset_index(drop=True)\n",
    "        TrainGlobalConfig.valset_size = len(valid_df)\n",
    "\n",
    "        train_dataset = Cassava_dataset(TrainGlobalConfig,\n",
    "                                        train_df,\n",
    "                                        transforms=get_train_transforms(TrainGlobalConfig.image_size),\n",
    "                                        mode=\"train\")\n",
    "\n",
    "        val_dataset = Cassava_dataset(TrainGlobalConfig,\n",
    "                                      valid_df,\n",
    "                                      transforms=get_valid_transforms(TrainGlobalConfig.image_size),\n",
    "                                      mode=\"val\")\n",
    "\n",
    "        run_training(net=net, train_dataset=train_dataset, validation_dataset=val_dataset, config=TrainGlobalConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
